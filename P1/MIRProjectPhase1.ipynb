{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTWsrYobJnd3"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=30>\n",
    "<p></p><p></p>\n",
    "به نام خدا\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<img src=\"Images/sharif.png\" width=\"25%\">\n",
    "<font color=blue>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=green>\n",
    "فاز اول پروژه - سیستم بازیابی اطلاعات داده‌های ویکی‌پدیای فارسی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=#FF7500>\n",
    "بهار ۹۹\n",
    "<br>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXCFFyc8Jnd7"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مقدمه </div>\n",
    "</font>\n",
    "<hr>\n",
    "در فاز اول پروژه درس بازیابی پیشرفته اطلاعات، شما باید سیستم بازیابی اطلاعات را برای مجموعه داده‌های ویکی پدیای فارسی پیاده سازی کنید. بدین صورت که مجموعه داده‌هایی که در اختیارتان قرار داده شده را پس از پردازش اولیه و نمایه‌سازی، آماده جستجو عبارات در آن کنید. سعی شده‌است که امکانات خواسته شده در این سیستم متناسب با جست‌وجو‌های کاربردی بر روی داده‌ها باشد.\n",
    "<br>\n",
    "پروژه از ۴ بخش تشکیل شده،‌ بخش اول آن آماده‌سازی اولیه داده‌هاست. پیشنهاد می شود برای پیاده‌سازی این بخش از کتابخانه هضم که توضیحات استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است، استفاده کنید. بخش دوم، طراحی و پیاده‌سازی نمایه‌ساز برای داده‌هاست که با گرفتن داده‌های ورودی، نمایه‌ها و داده‌ساختار‌های مورد نیاز برای جستجو اسناد و دیگر نیازمندی‌های سیستم را تولید می‌کند. در بخش سوم می‌بایست امکان جستجو بر روی داده‌ساختار خروجی بخش قبلی را براساس مدل فضای برداری فراهم کنید. در این قسمت عبارت مورد جستجو در صورت دارا بودن غلط املایی باید اصلاح شود. در بخش آخر نیز با استفاده از پرسمان‌ها و اسنادی که به عنوان اسناد مرتبط به آن پرسمان معرفی شده، می‌بایست سیستم بازیابی خود را با استفاده از ۴ معیار ذکر شده در این بخش\n",
    "ارزیابی کنید.\n",
    "<br>\n",
    "در این دفترچه جوپیتر برای هر یک از چهار بخش پروژه، قسمت مجزایی در نظر گرفته شده‌است. شما باید کدهای خود را طوری بزنید که این بخش‌ها طبق توضیح به تفضیل آمده در هر بخش، به درستی کار کنند. کد‌های خود را می‌توانید در بخش‌های اضافه شده توسط خودتان در همین دفترچه جوپیتر بنویسید یا فایل‌های پایتون مربوط به پیاده‌سازی خود را در کنار دفترچه گذاشته و در بخش‌های مختلف این دفترچه بااستفاده از \n",
    "import\n",
    "مناسب از کد‌هایتان استفاده کنید.\n",
    "<br>\n",
    "در نهایت توجه کنید که دو بخش از این فاز پروژه به عنوان قسمت امتیازی برای شما در نظر گرفته شده. در این سند، بخش‌های امتیازی با علامت (*امتیازی*) مشخص شده‌اند. هر کدام از این بخش ها 10 نمره دارند.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-nJxAxdJnd8"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مجموعه دادگان</div>\n",
    "</font>\n",
    "<hr>\n",
    "مجموعه دادگان مورد استفاده در این پروژه از جمع آوری اطلاعات موجود در صفحات ویکی پدیای فارسی به وجود آمده است.\n",
    "این مجموعه اسناد از دو بخش تشکیل شده است\n",
    ".\n",
    "<br>\n",
    "بخش اول که در فایل \n",
    "Persian.xml\n",
    "آمده است، شامل ۱۵۰۰ سند می‌باشد.\n",
    "هر سند شامل شناسه\n",
    "(id)،\n",
    "عنوان\n",
    "(title)،   \n",
    "و متن \n",
    "(text)\n",
    "است.\n",
    "بخش دوم که در پوشه‌ی \n",
    "queries\n",
    "آمده‌است، شامل تعدادی پرسمان است که برای سنجش سیستم‌ پیاده سازی شده‌ی شما مورد استفاده قرار خواهد گرفت.\n",
    "بخش سوم که در پوشه‌ی\n",
    "relevance\n",
    "آمده‌است،\n",
    "شامل یک فایل است که شناسه سند‌های مرتبط با هر پرسمان در آن آمده‌است.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avT4ky8EJnd-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" ><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>(10 نمره) بخش اول: آماده‌سازی اولیه داده‌ها</div>\n",
    "</font>\n",
    "<hr>\n",
    "هدف از این بخش اعمال عملیات متنی اولیه بر روی متن خام ورودی است تا کلمات به شکل مناسب برای قرارگیری در نمایه استخراج شوند. برای تسهیل این بخش شما می‌توانید از توابع کتابخانه‌ی هضم که توضیح استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است استفاده‌ نمایید. همین طور در صورت نیاز به توضیحات بیشتر در خصوص این کتاب‌خانه می‌توانید به توضیحات مربوط به پروژه‌ی سه سال قبل از طریق\n",
    "<a href=\"http://ce.sharif.edu/courses/95-96/1/ce324-1/assignments/files/assignDir/MIR_Project1.pdf\">این صفحه</a>\n",
    "مراجعه کنید.\n",
    "<br>\n",
    "<br>\n",
    "عملیات مورد انتظار:\n",
    "<ol>\n",
    "    <li>\n",
    "یکسان‌سازی متن: یکی از عملیات مهم در پردازش متون به خصوص در زبان فارسی این عملیات\n",
    "است که شامل یکسان‌سازی استفاده از فاصله و نیم‌فاصله و نحوه‌ی شکستن یا ادغام کلمات و ... است.  به طور مثال، یک مورد از این یکسان‌سازی‌ها نحوه‌ی قرار گیری حرف جمع «ها» در انتهای کلمات جمع است که می‌تواند بدون فاصله چسبیده به کلمه، با یک فاصله‌ی کامل و یا با نیم‌فاصله\n",
    "پس از کلمه بیاید (کتابها، کتاب ها، کتاب‌ها)\n",
    "    </li>\n",
    "    <li> \n",
    "جدا کردن کلمات یک جمله: واحد متن مورد استفاده‌ی ما در ساخت نمایه و همین طور جست‌وجو در یک سیستم اطلاعاتی کلمات هستند. بنابر این جملات ورودی را باید بتوانیم به کلمات آن بشکنیم  و عملیات مورد نیاز را بر روی کلمات انجام دهیم.\n",
    "    </li>\n",
    "    <li>\n",
    "حذف علائم نگارشی: علائم نگارشی مانند نقطه، ویرگول، و ... باید از درون اسناد حذف شوند تا درون نمایه و جست‌وجو‌ها تاثیر نگذارند.\n",
    "    </li>\n",
    "<li>\n",
    "بازگرداندن کلمات به ریشه: عملیات دیگری که روی کلمات متن صورت میگیرد عمل بازگردانی به\n",
    "ریشه\n",
    "(stemming)\n",
    "است تا کلماتی که از یک ریشه هستند همگی یک کلمه به حساب بیاید.\n",
    "    </li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzEhdYtzJnd_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در این بخش که برای آماده‌سازی اولیه متن داده‌هاست، تابع \n",
    "prepare_text\n",
    "باید طوری بر روی متن ورودی با نام\n",
    "raw_text\n",
    "عمل کند که\n",
    "عملیات‌های مورد انتظار ذکر شده روی متن انجام شود و متن آماده‌شده به عنوان خروجی تابع برگردانده شود. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ98SbQMJneB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text:\n",
      "['سلا', 'حال', 'خوب', 'اس', 'ممنون']\n"
     ]
    }
   ],
   "source": [
    "#!pip install hazm\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()\n",
    "tokenizer = WordTokenizer(join_verb_parts=True, replace_hashtags=True)    \n",
    "stemmer = Stemmer()\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    prepared_text = raw_text\n",
    "    \n",
    "    prepared_text = normalizer.normalize(raw_text)\n",
    "    \n",
    "    prepared_words = tokenizer.tokenize(raw_text)\n",
    "    final_words = []\n",
    "    punctuations = ['؟', '?', '!', '!', '،', '.', '.', ':', '«', '»', '-', '_', '...', ']', '[', ')', ')', '(', '/', '؛', '،', '؟', '\"', '\\'', '', ' ', '',]\n",
    "    bad_tokens = ['TAG']\n",
    "    for i, word in enumerate(prepared_words):\n",
    "        if word not in punctuations and word not in bad_tokens:\n",
    "            final_words.append(word)\n",
    "\n",
    "    for i, word in enumerate(final_words):\n",
    "        f_word = stemmer.stem(word)\n",
    "        if len(f_word) > 0:\n",
    "            final_words[i] = f_word\n",
    "\n",
    "    return final_words\n",
    "\n",
    "print('Enter text:')\n",
    "raw_text = 'سلام، حالت خوب است؟ ممنونم.'\n",
    "print(prepare_text(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "mlwmeLaQJneG"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (30 نمره) بخش دوم: ساخت نمایه</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این بخش شما باید نمایه‌گذاری‌های مورد نیاز برای بخش جست‌وجو را انجام دهید. تمامی نمایه‌ها باید به صورت پویا باشند به این معنی که با حذف و یا اضافه کردن سندی در طول اجرای برنامه، سند از نمایه حذف شده و یا به آن اضافه شود. \n",
    "<br>\n",
    "شرح نمایه‌های مورد انتظار:\n",
    "<br>\n",
    "<ol>\n",
    "<li>\n",
    "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا متن آن، در این قسمت بایستی نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید. با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه جایگاه‌های این کلمه در هر بخش هر سند را پیدا کرد. انتخاب داده‌ساختار مناسب برای ذخیره نمایه بر عهده خودتان است\n",
    "(البته روش استفاده شده باید مبتنی بر موارد معرفی شده در کلاس باشد.).\n",
    "همچنین باید قادر باشید نمایه‌ها را در فایلی ذخیره کرده و از فایل ذخیره شده بازیابی کنید\n",
    "</li>\n",
    "<li>\n",
    "(*امتیازی*)\n",
    "نمایه‌ی \n",
    "Bigram: \n",
    "با استفاده از این نمایه می‌توان با دادن یک \n",
    "Bigram\n",
    "(ترکیب‌های دو حرفی) \n",
    "تمامی کلمات موجود در لغتنامه که این ترکیب در آنها موجود است را دریافت کرد. این نمایه برای قسمت اصلاح پرسمان که در بخش بعد توضیح داده خواهد شد، مورد استفاده قرار خواهد گرفت. توجه کنید که با حذف یک سند، تمامی کلمات موجود در آن از لغتنامه حذف نمی‌شوند زیرا ممکن است که آن کلمه در سند دیگری نیز آمده باشد. حذف یک کلمه را در صورتی انجام دهید که لیست آن در نمایه‌ی قسمت قبل خالی شده باشد.\n",
    "</li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjpoyRv9JneH"
   },
   "source": [
    " <div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\">\n",
    " <font face=\"XB Zar\" size=5>\n",
    "این بخش مربوط به ساخت نمایه‌هاست. تابع \n",
    "construct_indexes\n",
    "با گرفتن مسیر مجموعه‌داده‌ها\n",
    "اقدام به ساختن دو نمایه‌ی شرح داده‌شده می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_akK-pvJneI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1572/1572 [02:31<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def make_dict(index, bi_index, words, doc_id, part):\n",
    "    for i, word in enumerate(words):\n",
    "#         bigram index\n",
    "        word_bi = '$'+word+'$'\n",
    "        for j in range(len(word_bi)-1):\n",
    "            bi = word_bi[j] + word_bi[j+1]\n",
    "            if bi in bi_index.keys():\n",
    "                list_of_words = bi_index[bi]\n",
    "                if word not in list_of_words:\n",
    "                    list_of_words.append(word)\n",
    "            else:\n",
    "                bi_index[bi] = [word]\n",
    "        \n",
    "#         main index\n",
    "        if word not in index.keys():\n",
    "            index[word] = {doc_id: {part: [i]}}\n",
    "        else:\n",
    "            word_dict = index[word]\n",
    "            if doc_id not in word_dict.keys():\n",
    "                word_dict[doc_id] = {part: [i]}\n",
    "            else:\n",
    "                if part in word_dict[doc_id]:\n",
    "                    part_list = word_dict[doc_id][part]\n",
    "                    part_list.append(i)\n",
    "                    word_dict[doc_id][part] = part_list\n",
    "                else:\n",
    "                    word_dict[doc_id][part] = [i]\n",
    "            index[word] = word_dict\n",
    "            \n",
    "\n",
    "def construct_positional_indexes(docs_path):\n",
    "    global N\n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.parse(docs_path)\n",
    "    root = tree.getroot()\n",
    "    index = {}\n",
    "    bi_index = {}\n",
    "    for child in tqdm(root):\n",
    "        title = child.find('{http://www.mediawiki.org/xml/export-0.10/}title').text\n",
    "        doc_id = child.find('{http://www.mediawiki.org/xml/export-0.10/}id').text\n",
    "        text = child.find('{http://www.mediawiki.org/xml/export-0.10/}revision').find('{http://www.mediawiki.org/xml/export-0.10/}text').text\n",
    "        title_words = prepare_text(title)\n",
    "        text_words = prepare_text(text)\n",
    "        N += 1\n",
    "        make_dict(index, bi_index, title_words, doc_id, 'title')\n",
    "        make_dict(index, bi_index, text_words, doc_id, 'text')\n",
    "    return index, bi_index\n",
    "\n",
    "N = 0\n",
    "index, bi_index = construct_positional_indexes('data/Persian.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51VMBVg3JneM"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای مشاهده \n",
    "posting list\n",
    "یک کلمه و جایگاه‌های کلمه در هر بخش سند (عنوان و متن) است. تابع\n",
    "get_posting_list\n",
    "با گرفتن\n",
    "word\n",
    "به عنوان کلمه ورودی، یک دیکشنری به عنوان خروجی بر می‌گرداند که کلید‌های دیکشنری شناسه سند‌هایی است که کلمه در آن وجود داشته‌است.\n",
    "    برای هر شناسه سند آمده در کلید‌های دیکشنری، یک دیکشنری به عنوان مقدار وجود خواهد داشت که کلید‌های آن می‌تواند \n",
    "title\n",
    "و\n",
    "text\n",
    "باشد که جایگاه‌های آمدن کلمه در بخش‌های عنوان و متن به صورت لیست به عنوان مقدار هر یک از این کلید‌ها می‌آید. به طور مثال اگر یک کلمه مثل «سلام» در سند‌۱۰ در جایگاه ۲ عنوان و جایگاه‌های ۴ و ۸ متن و در سند ۲۹ در جایگاه ۱۹ متن آمده باشد دیکشنری به صورت آمده در قطعه کد زیر خواهد بود\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLnvKnQ0JneN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3072': {'text': [1019]},\n",
       " '3260': {'text': [3072, 3086]},\n",
       " '3359': {'text': [1959]},\n",
       " '3414': {'text': [3154]},\n",
       " '3785': {'text': [893]},\n",
       " '3853': {'text': [145, 202]},\n",
       " '3854': {'title': [0],\n",
       "  'text': [287, 359, 395, 459, 479, 495, 749, 761, 769, 807, 818, 823]},\n",
       " '4002': {'text': [2948, 2954, 3015]},\n",
       " '6162': {'text': [341]},\n",
       " '6589': {'text': [435]},\n",
       " '6604': {'text': [80]},\n",
       " '6969': {'text': [4033]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_posting_list(word):\n",
    "    return index[word]\n",
    "\n",
    "get_posting_list('عجایب')\n",
    "# get_posting_list(prepare_text('هیلاندراس')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__qCYhAsJneS"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای مشاهده تمام کلماتی است که دارای یک دوحرفی خاص درون خود هستند. تابع \n",
    "get_words_with_bigram\n",
    "یک ورودی به عنوان\n",
    "bigram\n",
    "می‌گیرد و تمام کلماتی را که دارای این دو حرفی هستند به عنوان خروجی بر می‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6qxX9KAJneT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|میانگین\\u200cبارش\\u200cسالانه=',\n",
       " 'تلاق',\n",
       " 'ساوجبلاغ',\n",
       " 'سابلاغ',\n",
       " 'ولا',\n",
       " 'اعلا',\n",
       " 'اسقلال',\n",
       " 'میلاد',\n",
       " 'دلال',\n",
       " 'لاز',\n",
       " 'خلاصه',\n",
       " 'علاقه',\n",
       " 'قبلاً',\n",
       " 'علاوه',\n",
       " \"'''لاز\",\n",
       " 'اطلاعات',\n",
       " 'اطلاع',\n",
       " 'علام',\n",
       " 'بالا',\n",
       " 'طولانی\\u200cمد',\n",
       " 'علاقه\\u200cمند',\n",
       " 'اولاد',\n",
       " 'استقلال',\n",
       " 'اسلام',\n",
       " 'هلال',\n",
       " 'انقلاب',\n",
       " \"''ولایة''\",\n",
       " 'سنگلاخ',\n",
       " 'الان',\n",
       " 'اسلا',\n",
       " 'الاستعمار',\n",
       " 'خلاف',\n",
       " 'الاستعماریه',\n",
       " 'سالانه',\n",
       " 'الانوار',\n",
       " 'سطح\\u200cبالا',\n",
       " 'مردم\\u200cسالاری|',\n",
       " 'بالاترین',\n",
       " 'کاملاً',\n",
       " 'الاسلا',\n",
       " 'اسلام\\u200cگرا',\n",
       " 'معمولاً',\n",
       " 'لاکرب',\n",
       " 'الاخضر|حزا',\n",
       " 'الاخضر',\n",
       " 'جبل\\u200cالاخضر',\n",
       " 'مسلاته',\n",
       " 'جبل\\u200cالاخضر|جبل\\u200cالاخضر',\n",
       " 'دلار',\n",
       " 'کالا',\n",
       " 'مبتلا',\n",
       " 'لاتین',\n",
       " 'اسلاو',\n",
       " 'مالاگا',\n",
       " 'لاذقیه',\n",
       " \"'''لاذَقیه'''\",\n",
       " \"عربی|'''اللاذقية'''\",\n",
       " \"لاتینی|''Laodicea\",\n",
       " 'للمیلاد',\n",
       " 'والاسلامیة',\n",
       " 'ایلا',\n",
       " 'اردلان',\n",
       " 'ایلام',\n",
       " 'اصطلاح',\n",
       " 'سلاطین',\n",
       " 'اطلاق',\n",
       " 'مثلاً',\n",
       " 'الاکراد</ref>',\n",
       " 'الارض',\n",
       " 'الانبیا',\n",
       " 'الانبیاء',\n",
       " 'الصلاه',\n",
       " 'السلا',\n",
       " 'الاکراد',\n",
       " 'ولادمیر',\n",
       " 'ملاطیه',\n",
       " 'احتمالاً',\n",
       " 'ملا',\n",
       " 'تلا',\n",
       " 'تازه\\u200cاستقلال\\u200cیافته',\n",
       " 'اختلاف',\n",
       " 'غلامرضا',\n",
       " 'باجلانی',\n",
       " 'ولادیمیر',\n",
       " 'اطلاعات\\u200cنامه',\n",
       " 'طولان',\n",
       " 'ایلام|ایلا',\n",
       " 'ملاحظه',\n",
       " 'دالاهو',\n",
       " 'اختلاط',\n",
       " 'اخلاف',\n",
       " 'حاصلخیز|هلال',\n",
       " 'اسلام|ناشر',\n",
       " 'ایلام|سال=',\n",
       " 'هه\\u200cلاله',\n",
       " 'px|هلاله',\n",
       " 'هلاله',\n",
       " 'دلاور',\n",
       " 'سالار',\n",
       " 'لایق',\n",
       " 'جلال',\n",
       " 'پلاک',\n",
       " 'سونگورلاره',\n",
       " 'زلاندنو',\n",
       " 'لارنس|رودخانه',\n",
       " 'لارنس',\n",
       " 'خلال',\n",
       " 'میلادی-',\n",
       " 'زلاند',\n",
       " 'ملاق',\n",
       " 'سولاندر',\n",
       " 'بلافاصله',\n",
       " 'عملاً',\n",
       " 'تلاش',\n",
       " 'کیالاکه\\u200cکوا',\n",
       " 'دلایل',\n",
       " 'آلاسکا',\n",
       " 'سلاح',\n",
       " 'کلاسیک|کلاسیک',\n",
       " 'اصلاح',\n",
       " 'شده|اصلاح',\n",
       " 'بعلاوه',\n",
       " 'ایرلاینز',\n",
       " 'بلاویا',\n",
       " 'فلای\\u200cدب',\n",
       " 'ایرلاینز|SCAT',\n",
       " 'لارناکا',\n",
       " 'آمریکا|دلار',\n",
       " 'بلامانع',\n",
       " '|سال\\u200cولایت\\u200cشدن=',\n",
       " '|علاقه\\u200cداری\\u200cها=',\n",
       " 'ولایت',\n",
       " 'مواصلات',\n",
       " 'بالامرغاب',\n",
       " 'الایا',\n",
       " 'فلادلفیا',\n",
       " 'ولایات',\n",
       " 'اصلا',\n",
       " 'الاخبار',\n",
       " 'حالا',\n",
       " 'بالاخره',\n",
       " \"'''بَغلان'''\",\n",
       " 'مشهورولا',\n",
       " 'فعلاً',\n",
       " 'بلخی|مولانا',\n",
       " 'مولانا',\n",
       " 'غلام\\u200cحضر',\n",
       " \"'''ولا\",\n",
       " 'px|ولا',\n",
       " '|ولایت=',\n",
       " 'پلانگذار',\n",
       " 'آلات',\n",
       " 'سلام',\n",
       " 'عبدالاحمد',\n",
       " 'مرکزولا',\n",
       " 'تغییرمسیر|انقلاب',\n",
       " \"'''انقلاب\",\n",
       " \"اسلامی'''\",\n",
       " 'سکولار',\n",
       " 'اسلامی</ref><ref>',\n",
       " 'اسلامی</ref>',\n",
       " 'لاهیج',\n",
       " 'طولانی',\n",
       " '</ref><ref>میلان',\n",
       " 'دیوانسالار',\n",
       " 'اصلاح\\u200cاند',\n",
       " 'علامه',\n",
       " 'میلان',\n",
       " 'پک|میلانی|',\n",
       " 'اصلاح\\u200cطلب',\n",
       " 'انقلابیون',\n",
       " 'کلاس',\n",
       " 'استدلال',\n",
       " 'برخلاف',\n",
       " 'طلاب',\n",
       " 'اعلامیه',\n",
       " 'اصلاح\\u200cنشده',\n",
       " 'اسلامگرا',\n",
       " 'خلاص',\n",
       " 'انحلال',\n",
       " 'بی\\u200cملاحظه',\n",
       " 'اصلاحگر',\n",
       " 'ارباب\\u200cسالار',\n",
       " 'گلایه',\n",
       " 'اطلاع\\u200cرسان',\n",
       " 'کلانتری',\n",
       " 'پالایشگاه',\n",
       " 'ائتلاف',\n",
       " 'لاهوت',\n",
       " 'اسلامی|کُمیته',\n",
       " 'انقلاب|دادگاه',\n",
       " 'ایران|انقلاب',\n",
       " 'ابلاغ',\n",
       " 'تشکیلات',\n",
       " 'ممنوع\\u200cالانتشار',\n",
       " 'اختلال',\n",
       " 'ملاز',\n",
       " 'فی\\u200cالارض',\n",
       " 'com/fa-ir/واژههایی-که-با-جمهوری-اسلامی-به-ایران-آمدند/g-',\n",
       " 'لاله',\n",
       " \"انقلاب'''\",\n",
       " 'آمریکا|اعلا',\n",
       " 'بالا</font>',\n",
       " 'لاتینو',\n",
       " 'گواتمالا',\n",
       " 'دلاویر',\n",
       " 'آمریکا|اعلامیه',\n",
       " 'فیلادلفیا',\n",
       " 'اصلاحیه',\n",
       " 'متقابلاً',\n",
       " 'ونزوئلا',\n",
       " 'لاب',\n",
       " 'دالاس',\n",
       " '|کلان\\u200cشهر',\n",
       " 'لانگ',\n",
       " 'آمریکا|آتلانتیک',\n",
       " 'دالاس-فور',\n",
       " 'لادردیل',\n",
       " 'آتلانتا',\n",
       " 'کلانشهر',\n",
       " 'بلاعوض',\n",
       " 'اقلا',\n",
       " 'میلادی</ref>',\n",
       " 'فولاد',\n",
       " 'پالا',\n",
       " 'دلار<sup>',\n",
       " 'سکولار|کشور',\n",
       " 'سی-لا',\n",
       " 'بنگلاد',\n",
       " 'خلاق',\n",
       " 'گولا',\n",
       " 'کلان',\n",
       " 'بالاتر',\n",
       " \"''فیلادلفیا\",\n",
       " 'لاابال',\n",
       " \"الافکار''\",\n",
       " 'اطلال',\n",
       " 'نیوکلاسیک',\n",
       " 'غلامحسن',\n",
       " 'کلاسیک',\n",
       " 'جلال\\u200cالدین',\n",
       " 'جلال\\u200cالدّین',\n",
       " 'بلاغ',\n",
       " 'غلامحسین',\n",
       " 'مثلااستخو',\n",
       " 'گلاب',\n",
       " 'سه\\u200cلایه\\u200cازجنس',\n",
       " 'غیراخلاق',\n",
       " 'اخلاق',\n",
       " 'تالار',\n",
       " 'اولاند',\n",
       " 'htm<small>اطلاع',\n",
       " '>بالا</font>',\n",
       " 'لاسکو',\n",
       " 'آتلانتیک',\n",
       " 'نیکولا',\n",
       " 'پرلاشز',\n",
       " 'پالاس',\n",
       " 'لاویل',\n",
       " 'فنلاند',\n",
       " 'دلار<ref',\n",
       " '>بالا</span>',\n",
       " 'ژولا',\n",
       " 'لاتویا',\n",
       " 'بلاروس',\n",
       " 'سالانه\\u200cاس',\n",
       " 'اصلاحات',\n",
       " 'صلاح',\n",
       " 'عادلانه',\n",
       " 'ناعادلانه',\n",
       " 'کلا',\n",
       " 'موزیلا',\n",
       " \"'''موزیلا'''\",\n",
       " 'خلاء',\n",
       " 'پلانک',\n",
       " 'آلاینده',\n",
       " 'میلاد<ref',\n",
       " 'طلا',\n",
       " 'لا',\n",
       " 'کلاه',\n",
       " 'علائ',\n",
       " 'عیلام',\n",
       " 'ایلامی',\n",
       " 'کلاه\\u200cتیزخود',\n",
       " 'ملازم',\n",
       " 'لاجورد',\n",
       " 'دالانِ',\n",
       " 'والا',\n",
       " 'املا',\n",
       " 'آنلاین',\n",
       " '|انقلاب',\n",
       " 'لاژورد',\n",
       " 'دولاب',\n",
       " \"روزآنلاین'''\",\n",
       " 'اسلامیکا|تاریخ=|تاریخ',\n",
       " '|لاتین',\n",
       " 'نیلاندر',\n",
       " 'ملک\\u200cشاه|جلال\\u200cالدین',\n",
       " 'جلالی|تقو',\n",
       " '<ref>علاّمه',\n",
       " 'میلادی|',\n",
       " 'همشهری\\u200cآنلاین|نشان',\n",
       " 'اسلامی|نشان',\n",
       " 'کارلا',\n",
       " 'سلا',\n",
       " 'غلا',\n",
       " 'سالار|مرد',\n",
       " 'اطلاع\\u200cرسانی</ref>',\n",
       " 'بلال',\n",
       " 'ربیع\\u200cالاول',\n",
       " 'خلافت',\n",
       " 'محمد|خلاف',\n",
       " 'عاملان',\n",
       " 'جمادی\\u200cالاول',\n",
       " 'نهج\\u200cالبلاغه',\n",
       " 'اسلام|Encyclopaedia',\n",
       " 'الامک',\n",
       " 'موزیلا|ام\\u200cپی\\u200cال',\n",
       " \"'''موزیلا\",\n",
       " 'مشکلات',\n",
       " 'پلاگین',\n",
       " 'غلاف',\n",
       " 'هسته\\u200cای|سلاح',\n",
       " 'بالاس',\n",
       " 'قلاع',\n",
       " 'کهنسالان',\n",
       " 'سیلاخور',\n",
       " 'واژگون|لاله',\n",
       " 'دالان',\n",
       " 'قزل\\u200cآلا',\n",
       " 'خاتم\\u200cالانبیا',\n",
       " 'نیکلا',\n",
       " 'رومی|جلال',\n",
       " 'بالاب',\n",
       " 'پالاینده',\n",
       " 'پالایه',\n",
       " 'طلال',\n",
       " 'الاقص',\n",
       " 'لاتین|لاتین',\n",
       " 'سینالا',\n",
       " 'اصطلاحاً',\n",
       " 'معمولا',\n",
       " 'چهارلا',\n",
       " 'سه\\u200cلا',\n",
       " 'گلابی\\u200cشکل',\n",
       " 'پلاستیک',\n",
       " 'لایه',\n",
       " '٫۶||۵٫۴||چهارلا',\n",
       " '٫۷||۷٫۳||چهارلا',\n",
       " '٫۳||چهارلا',\n",
       " '٫۹||چهارلا',\n",
       " '٫۵||چهارلا',\n",
       " 'لابمل',\n",
       " 'لاکرن',\n",
       " '٫۶||چهارلا',\n",
       " '٫۰||چهارلا',\n",
       " '٫۷||چهارلا',\n",
       " '٫۲||چهارلا',\n",
       " 'نت|لا',\n",
       " \"لا''\",\n",
       " 'اسلام|مسلمان',\n",
       " 'میلادی|طرح',\n",
       " 'سکولاریسم|سیس',\n",
       " 'سکولاریس',\n",
       " 'موتورولا',\n",
       " 'محصولات',\n",
       " 'کلاهک',\n",
       " 'حملات',\n",
       " 'مسئولانه',\n",
       " 'لاتینی\\u200cساز',\n",
       " 'استیلا',\n",
       " 'لاییک',\n",
       " 'آلبانی|استقلال',\n",
       " 'مالاکاس',\n",
       " \"مالاکاستر'''\",\n",
       " 'لابئ',\n",
       " 'تاؤلان',\n",
       " 'باهیابلانکا',\n",
       " 'پلاتا',\n",
       " 'لاپلاتا',\n",
       " 'آدلاید',\n",
       " 'براتیسلاوا',\n",
       " 'ماچالا',\n",
       " 'لاشنجه',\n",
       " 'لایپزیگ',\n",
       " 'آنگولا',\n",
       " 'کامپالا',\n",
       " 'لاس',\n",
       " 'اکلاهماسیت',\n",
       " 'گلاسگو',\n",
       " 'لاپاز',\n",
       " 'لاهور',\n",
       " 'پوکاپالا',\n",
       " 'کالائو',\n",
       " 'دارالسلا',\n",
       " 'لانژو',\n",
       " 'ولادی\\u200cوستوک',\n",
       " 'سریلانکا',\n",
       " 'کالاما',\n",
       " 'سلاله',\n",
       " 'بارانکیلا',\n",
       " 'سانتاکلارا',\n",
       " 'دلاس',\n",
       " 'آخالکالاک',\n",
       " 'تلاو',\n",
       " 'مالابو',\n",
       " 'لائوس',\n",
       " 'مالاو',\n",
       " 'کوالالامپور',\n",
       " 'کازابلانکا',\n",
       " 'اولانباتار',\n",
       " 'گوادالاخارا',\n",
       " 'لاگوس',\n",
       " 'پورت\\u200cویلا',\n",
       " 'لاهه',\n",
       " 'سولا',\n",
       " 'علائم',\n",
       " 'اختلالات',\n",
       " 'اسلاو|اسلاو',\n",
       " 'الاءعلاق\\u200cالنفیسة',\n",
       " 'افلاک',\n",
       " 'لان',\n",
       " \"'''لانتان'''\",\n",
       " 'لانتانید',\n",
       " '|ولایت=هلمند',\n",
       " '|علاقه\\u200cداری=',\n",
       " 'اتولا',\n",
       " 'ملاهاد',\n",
       " 'خانم\\u200cبالا',\n",
       " 'طلاق',\n",
       " 'بلا',\n",
       " 'لیلا',\n",
       " 'غلامحسین\\u200cمیرزا',\n",
       " 'جلال\\u200cالممالک',\n",
       " 'تحصیلات',\n",
       " 'لاغراندا',\n",
       " 'خلق|طلاب',\n",
       " 'لابه\\u200cلا',\n",
       " 'گیلان',\n",
       " 'کلاغ',\n",
       " 'روز-مولاناست-ولی-انگار-نه-انگار|عنوان=روز',\n",
       " 'مولاناس',\n",
       " 'میرعلا',\n",
       " 'کلام',\n",
       " 'زرکوب|صلاح',\n",
       " 'الا',\n",
       " 'اولاً',\n",
       " \"استعلامی'''\",\n",
       " \"'''جواهرالاسرار\",\n",
       " \"زواهرالانوار'''\",\n",
       " \"'''فاتح\\u200cالابیات'''\",\n",
       " 'استعلام',\n",
       " \"علا'''\",\n",
       " 'به\\u200cحلال',\n",
       " 'سپه\\u200cسالار',\n",
       " 'لاهج',\n",
       " 'دلايل-هشت-گانه-ضرورت-تغيير-پول-ملي',\n",
       " 'لاین',\n",
       " 'لاین|پالیزه',\n",
       " 'جولانگاه',\n",
       " 'کلاسیس',\n",
       " 'کلاسیسیس',\n",
       " 'بالائ',\n",
       " 'هیمالایا',\n",
       " 'ماکس\\u200cپلانک',\n",
       " 'داگلاس',\n",
       " 'متلاش',\n",
       " 'الارب',\n",
       " 'کلارک',\n",
       " '<ref>نولا',\n",
       " 'کالا</ref>',\n",
       " 'کلادون',\n",
       " 'کولادون',\n",
       " 'ملاً',\n",
       " 'پلاستک',\n",
       " 'نورلامپ',\n",
       " \"'''موادلاز\",\n",
       " 'لایه\\u200cنشان',\n",
       " 'لاشار|لاشار',\n",
       " 'لاشار',\n",
       " 'الاول',\n",
       " 'فلامینگو',\n",
       " 'وکلا',\n",
       " 'علاق',\n",
       " 'الإسلا',\n",
       " 'اسلام|اسلام',\n",
       " 'آلمان|آلمانی\\u200cالاصل',\n",
       " 'ملاءعا',\n",
       " 'صالح|اسلا',\n",
       " 'بالارتبه',\n",
       " '=شلاق',\n",
       " 'لاور',\n",
       " 'الاسلام-',\n",
       " 'ملایر',\n",
       " 'ملامحمد',\n",
       " 'لائودیسه',\n",
       " 'لااودیسه',\n",
       " 'فتوکلاژ',\n",
       " '|میانگین\\u200cبارش\\u200cسالانه',\n",
       " 'کلان\\u200cشهر',\n",
       " 'لاهیجانی|',\n",
       " 'لاهیجان',\n",
       " 'خلایق',\n",
       " 'الاشرف',\n",
       " 'کربلا',\n",
       " 'معلا',\n",
       " 'الاقالیم|احسن',\n",
       " 'الاسفزار',\n",
       " 'دارالخلافه',\n",
       " 'الاقال',\n",
       " 'اسلام|ص=',\n",
       " 'ایرانی-اسلام',\n",
       " 'گیلاس',\n",
       " 'اعتلاء',\n",
       " 'اعتلا',\n",
       " 'اسلامی-مدرن',\n",
       " 'نشانه-های-اسلامی-در-معماری-و-شهرسازی-مشهد-رعایت-نمی-شود|',\n",
       " 'بازنگری-معماری-و-شهرسازی-کلانشهر-مشهد-لزوم-تبدیل-به-پایتخت-فرهنگی-جهان-اسلا',\n",
       " 'اسلام|تاریخ=',\n",
       " 'سیلاب',\n",
       " 'بالاسر',\n",
       " \"قول|<small>'''برخلاف\",\n",
       " 'پالاندوز',\n",
       " 'ییلاق',\n",
       " '-ییلاق',\n",
       " 'محلات|ناشر=',\n",
       " 'دارالولایه',\n",
       " 'جوادالائمه',\n",
       " 'خاتم\\u200cالانبیا|بیمارس',\n",
       " 'خاتم\\u200cالانبیاء',\n",
       " 'آنلاین|تاریخ',\n",
       " 'اسلامی|تاریخ',\n",
       " 'اجلاس',\n",
       " 'مشهد-پایتخت-فرهنگی-جهان-اسلام-می-شود-فرصتی-ناب-برای-ترویج-فرهنگ|',\n",
       " 'اسلامی|بنیاد',\n",
       " 'ثامن\\u200cالائمه',\n",
       " 'کلانتر',\n",
       " 'آنلاین|تاریخ=',\n",
       " 'آنلاین|',\n",
       " 'کلات<ref>',\n",
       " 'کلات|',\n",
       " '-درصدی-مشکلات-زیست-محیطی-در-مشهد|',\n",
       " 'دلايل-گسترش-حاشیه-نشینی-در-مشهد-تدوین-سند-توانمندسازی-سکونتگاه-های|',\n",
       " '-افزایش-درصدی-طلاق-در-مشهد|',\n",
       " '-درصد-طلاق-ها-در-مشهد-است-راه-های-افزایش-کیفیت-رابطه-جنسی|',\n",
       " '-بعد-از-اعتیاد-طلاق-خشونت-سومین-آسیب-عمده-اجتماعی-در-مشهد|',\n",
       " 'مشهد-و-لاهور-خواهر-خوانده-شدند',\n",
       " 'کمپوستلا',\n",
       " 'غلام\\u200cحسین',\n",
       " 'باب\\u200cالابواب',\n",
       " 'آنلاین</ref><ref>',\n",
       " 'سولاک',\n",
       " 'اصلی|اختلاف',\n",
       " 'خبرآنلاین|تاریخ=',\n",
       " 'سفلا',\n",
       " 'اسلامی<',\n",
       " 'کشورعملا',\n",
       " 'پلانکتون',\n",
       " 'لاکادیو',\n",
       " 'لاکشادویپ|Laccadive',\n",
       " 'میلاً',\n",
       " 'سلاجقه',\n",
       " 'سَلاجقه',\n",
       " 'ارسلان|عضدالدوله',\n",
       " 'ملازگرد',\n",
       " 'ملکشاه|جلال\\u200cالدوله',\n",
       " 'الابنیه',\n",
       " 'الادویه',\n",
       " 'گیوه|کلا',\n",
       " '|پلاک',\n",
       " 'اردلان|اردلان',\n",
       " 'قشلاق',\n",
       " 'بلافصل',\n",
       " 'اسلام\\u200cآباد',\n",
       " 'اردلان|اردل',\n",
       " 'غلامشاه',\n",
       " 'شلا',\n",
       " 'گلابتون\\u200cدوز',\n",
       " 'هلاج',\n",
       " 'حلاج',\n",
       " 'ملاکاوو',\n",
       " 'کلاقاه',\n",
       " 'کلاو',\n",
       " 'پلان',\n",
       " 'امجدالاشراف',\n",
       " 'شیخ\\u200cالاسلا',\n",
       " 'ملالطف',\n",
       " 'قشلاق|پل',\n",
       " 'دارالاحس',\n",
       " 'دارالا',\n",
       " 'قدیم\\u200cالایا',\n",
       " 'قشلاق|سد',\n",
       " 'دارالایاله',\n",
       " 'محلات|مرکز',\n",
       " 'محلات',\n",
       " 'خبرآنلاین',\n",
       " 'محلات|',\n",
       " 'محلات|تاریخ',\n",
       " 'احتمالا',\n",
       " 'صدرالاشراف',\n",
       " 'محلاتی-و',\n",
       " '>علامه',\n",
       " 'الاعتقاد',\n",
       " 'الاسلام',\n",
       " 'فلاسفه',\n",
       " 'الاه',\n",
       " 'کلام|کلا',\n",
       " \"عقلانی'''\",\n",
       " '<ref>اخلاق',\n",
       " 'خلاقانه',\n",
       " 'سؤالات',\n",
       " 'جملات',\n",
       " 'بلایا',\n",
       " 'جلا',\n",
       " 'ولانس',\n",
       " 'جلالوند',\n",
       " 'کلاوه',\n",
       " 'بیلا',\n",
       " 'بلایای',\n",
       " 'سری\\u200cلانکا',\n",
       " 'لانکا',\n",
       " 'پالار|پالار',\n",
       " 'عیلا',\n",
       " 'هیولا',\n",
       " 'اصولاً',\n",
       " 'گیلان|گیل',\n",
       " 'فولادزره',\n",
       " 'بوسلامه',\n",
       " 'امیدسالار',\n",
       " 'الإسلامیة',\n",
       " 'ملاقات',\n",
       " 'آق\\u200cقلا',\n",
       " 'آق\\u200cقلا|آق\\u200cقلا',\n",
       " 'آلا',\n",
       " 'فلاورجان|فلاورج',\n",
       " 'ثلاث',\n",
       " 'باباجانی|ثلاث',\n",
       " 'ملایر|ملایر',\n",
       " 'غرب|اسلام\\u200cآباد',\n",
       " 'اسلام\\u200cشهر',\n",
       " 'اسلامشهر|اسلامشهر',\n",
       " 'اسلامیه',\n",
       " '|اسلامیه',\n",
       " 'لامرد|لامرد',\n",
       " 'اصلاندوز',\n",
       " 'امیرکلا',\n",
       " 'لارستان|لارس',\n",
       " 'آریاشهر|بالاده',\n",
       " 'دهگلان|دهگل',\n",
       " 'چالانچول',\n",
       " 'رستمکلا',\n",
       " 'سرخنکلاته',\n",
       " 'علامرود',\n",
       " 'فلاورج',\n",
       " 'فولادشهر',\n",
       " 'کلاته',\n",
       " 'کلاچا',\n",
       " 'کلارآباد',\n",
       " 'کلارد',\n",
       " 'کلاله',\n",
       " 'کیاکلا',\n",
       " '|کیاکلا',\n",
       " 'گیلانغرب',\n",
       " 'لار',\n",
       " 'لالجین',\n",
       " '|لاله',\n",
       " 'لال',\n",
       " 'لامرد',\n",
       " 'لاهرود',\n",
       " 'مرزیکلا',\n",
       " 'معلم\\u200cکلایه',\n",
       " 'ملاثان',\n",
       " 'ملارد',\n",
       " 'میلاجرد',\n",
       " 'جلایر',\n",
       " 'بلاذر',\n",
       " 'قشلاق|قشلاق',\n",
       " 'فاضلاب',\n",
       " 'گلابیِ',\n",
       " 'حلال',\n",
       " 'الامر',\n",
       " 'ملامراد',\n",
       " 'لایحضره',\n",
       " 'الانوارالنعمانیه',\n",
       " 'دررالاشعار',\n",
       " 'مصلا',\n",
       " 'کلاهخود',\n",
       " 'مولوی|مولانا',\n",
       " 'مولانا|شکل',\n",
       " 'الاصل',\n",
       " 'دلاویل',\n",
       " 'فیلارمونیک',\n",
       " \"مولانا''\",\n",
       " 'مولانا||',\n",
       " '||میلاد',\n",
       " 'لایق||||',\n",
       " 'ساپودیلا',\n",
       " 'اسطرلاب',\n",
       " 'زیلان',\n",
       " 'لاخ',\n",
       " 'دراکولا',\n",
       " '|دراکولا',\n",
       " 'کوپولا',\n",
       " 'دراکولا|دراکولا',\n",
       " 'لایکن',\n",
       " 'بالابردن',\n",
       " 'نیلا',\n",
       " \"''بالابلند\",\n",
       " \"بلندبالایی''\",\n",
       " \"''پائولا''\",\n",
       " 'وبلاگ',\n",
       " 'کتابلاگ',\n",
       " 'ملام',\n",
       " 'متلاط',\n",
       " 'آلاشکر',\n",
       " 'ارمنستان|آلاشکر',\n",
       " 'املائ',\n",
       " 'لازاریان|ژان',\n",
       " 'لازار',\n",
       " \"'''کیلان'''\",\n",
       " 'کالال',\n",
       " 'کالاآل',\n",
       " \"''کالاآل\",\n",
       " 'کالالیت',\n",
       " \"''توپیلاک''\",\n",
       " 'توپیلاک',\n",
       " 'کَلالیت*',\n",
       " 'کلال',\n",
       " 'پُردلانه',\n",
       " 'کَلال',\n",
       " 'اساس\\u200cالاقتباس',\n",
       " 'کلام|متکل',\n",
       " 'کلاما',\n",
       " 'کلام|کلام',\n",
       " 'اسلامی\\u200cس',\n",
       " 'هلاکو|هلاکو',\n",
       " 'هلاکو',\n",
       " 'اعلاء',\n",
       " 'الاعراق',\n",
       " 'هلاکوخ',\n",
       " 'البلاغه',\n",
       " 'البلاغة',\n",
       " 'الالقاب',\n",
       " 'رسالات',\n",
       " 'الاقتباس',\n",
       " 'ذیلاً',\n",
       " \"''اخلاق\",\n",
       " \"الاختیار''\",\n",
       " \"''خلاف\",\n",
       " \"''الاعتقادات''\",\n",
       " 'کلام|متکمین',\n",
       " ';کلا',\n",
       " \"''تجریک\\u200cالکلام''\",\n",
       " 'بروکلاین',\n",
       " 'بولاک',\n",
       " 'کلایو',\n",
       " 'اصلان',\n",
       " 'آن\\u200cلا',\n",
       " 'ستلا',\n",
       " '|گلاب',\n",
       " 'ماتلاک',\n",
       " 'پورتلاندیا',\n",
       " 'پدرسالار',\n",
       " 'دلایل-کناره-گیری-کتایون-ریاحی-از-بازیگری-سينما-آمده-تا-قداست-را-از-مذهب-و-عصمت-را-از-انبيا-بگيرد|عنو',\n",
       " 'بی\\u200cاطلاع',\n",
       " 'لاک\\u200cپ',\n",
       " '|لاک\\u200cپ',\n",
       " 'کارتالا',\n",
       " 'الببلاو',\n",
       " 'الازهر',\n",
       " 'اسلامی\\u200cساز',\n",
       " 'الاحمر',\n",
       " 'مردسالاری|مردسالار',\n",
       " 'اصلی|انقلاب',\n",
       " 'یونان|یونانی\\u200cالاصل',\n",
       " 'اصلاً',\n",
       " 'ارلاخ',\n",
       " 'ازمیلاد',\n",
       " 'کلاست',\n",
       " 'علاءالدین',\n",
       " 'علا\\u200cالدين',\n",
       " 'علاالدین',\n",
       " 'لاریج',\n",
       " 'com/جمهوری-اسلامی-ایران/مازندران/آمل/contents/',\n",
       " 'دیولافوا',\n",
       " 'مداخلات',\n",
       " 'گیلانی',\n",
       " 'com/مردمان-گیلان-و-مازندران-نژاد-کاس\\u200cپ',\n",
       " 'محتملاٌ',\n",
       " 'نگاهی-گذرا-به-سنت-ورف-چال-در-مازندران-زن-سالاری-یک-روزه-در',\n",
       " 'لاره',\n",
       " 'لاریجان',\n",
       " 'غلامعل',\n",
       " 'داعی\\u200cالاسلا',\n",
       " 'غلام',\n",
       " 'الاما',\n",
       " 'فخرالاسلا',\n",
       " 'علاو',\n",
       " 'چلاب',\n",
       " 'الاصغر',\n",
       " 'دلارستاق',\n",
       " 'ملاط',\n",
       " 'اضلاع',\n",
       " 'فیروزکلا',\n",
       " 'علاءالدوله',\n",
       " 'تلار',\n",
       " 'هندوکلا|تکیه',\n",
       " 'هندوکلا',\n",
       " 'قاضی\\u200cکلا',\n",
       " 'چلاو',\n",
       " 'آلاچیق',\n",
       " 'آلامل',\n",
       " 'پلاژ',\n",
       " 'میلاد|سه',\n",
       " 'com/مقالات/مدارس-نظاميه',\n",
       " 'کلاً',\n",
       " 'لاک',\n",
       " 'com/محصولات-مازندر',\n",
       " 'سلطان-مسعود-تعطیلاتش-را-چگونه-می\\u200cگذراند',\n",
       " 'تعطیلات',\n",
       " 'افلاطون',\n",
       " \"''افلاطون''\",\n",
       " 'پلاتون',\n",
       " \"'''افلاطون'''\",\n",
       " \"'''پلاتون'''\",\n",
       " 'افلاطون</ref>',\n",
       " 'افلاطون\\u200cشناس',\n",
       " 'گلاوکن',\n",
       " 'افلاطون\\u200cگرا',\n",
       " 'کولاب',\n",
       " 'لاهيج',\n",
       " 'شلایشر',\n",
       " 'کلارا',\n",
       " 'پاولا',\n",
       " 'هیتلر|پاولا',\n",
       " 'ایالات',\n",
       " 'منفعلانه',\n",
       " 'علایق',\n",
       " 'px|بندانگشتی|اعلا',\n",
       " 'قلاده',\n",
       " 'بلاند',\n",
       " '|بلاند',\n",
       " 'آنگلا',\n",
       " 'هیتلر|آنگلا',\n",
       " 'لابراتوار',\n",
       " 'اسلامی|کرج',\n",
       " 'بلاواتسک',\n",
       " 'ضداطلاع',\n",
       " 'فالانژیس',\n",
       " 'مسجدالاقص',\n",
       " 'لاشه',\n",
       " 'لازمه',\n",
       " 'کاملا',\n",
       " 'اسلامی-مجموعه',\n",
       " 'مقالات-ص',\n",
       " 'مشرق\\u200cالاذکار',\n",
       " 'استقلال\\u200cطلبانه',\n",
       " 'میلادی|ماه',\n",
       " 'گه\\u200cلاویژ',\n",
       " 'گه\\u200cلارێز',\n",
       " 'گه\\u200cلاوێژ',\n",
       " 'لاویژ',\n",
       " '||گه\\u200cلاوێژ',\n",
       " 'لائیک',\n",
       " 'کلاسوس',\n",
       " 'میلاد|پ',\n",
       " 'اسلامبول',\n",
       " 'سکولاریزم|اصل',\n",
       " 'مردمسالار',\n",
       " 'سکولاریسم|سکولار',\n",
       " \"'''هلال\",\n",
       " 'اسلامی-سن',\n",
       " 'آلانیا',\n",
       " 'گالاتاسرا',\n",
       " 'مالاتیا',\n",
       " 'موغلا',\n",
       " 'انگولا',\n",
       " 'گوآتمالا',\n",
       " 'فلاند',\n",
       " 'توکلائو',\n",
       " 'ملانز',\n",
       " 'پالائو',\n",
       " 'پولاد',\n",
       " 'هاسپولادل',\n",
       " 'باجلانی|زب',\n",
       " 'باجلان',\n",
       " 'مستقلاً',\n",
       " 'خلاصه\\u200cشده',\n",
       " 'لازلو',\n",
       " 'لاپتون',\n",
       " 'خلاقه',\n",
       " 'بلامنازع',\n",
       " 'علامت\\u200cگذاری',\n",
       " 'املاک',\n",
       " 'علامت',\n",
       " '|name=باجلان',\n",
       " \"'''باجلانی'''\",\n",
       " 'طلاساز',\n",
       " 'الازیغ|الازیغ',\n",
       " 'ایران|استیلا',\n",
       " 'لاشکار',\n",
       " 'لانه',\n",
       " 'آزاداسلام',\n",
       " 'سلامت',\n",
       " 'ازاسلا',\n",
       " 'جلال\\u200cآباد',\n",
       " 'لاله\\u200cزار',\n",
       " 'مولا',\n",
       " 'شهلا',\n",
       " '|دالاهو',\n",
       " 'متوسلان',\n",
       " 'کاپولا',\n",
       " 'الازیغ',\n",
       " 'ولایة',\n",
       " 'قیزلاردفه',\n",
       " 'غلات',\n",
       " 'آلاداغ',\n",
       " 'فالاریس',\n",
       " 'قلا',\n",
       " 'بولارسنگ|ایلده',\n",
       " 'بولارسنگ',\n",
       " 'بولارسنگ|قازانارسنگ',\n",
       " '|ولایت=کابل',\n",
       " 'کلاودیوس',\n",
       " 'کادفیزس|کوجولا',\n",
       " 'مایملاک',\n",
       " 'کالاله',\n",
       " 'علاقه\\u200cدار',\n",
       " 'بلاوقفه',\n",
       " 'ویلا',\n",
       " 'بالاحصار',\n",
       " 'عبدالسلا',\n",
       " \"'''بالا\",\n",
       " 'دهلا',\n",
       " 'بغلان|بغل',\n",
       " 'لابه',\n",
       " 'حلاو',\n",
       " 'مقالات',\n",
       " '|بالا',\n",
       " 'الگوب|اصلاح',\n",
       " \"|'''بالا'''\",\n",
       " \"|'''بالا\",\n",
       " \"|'''بالا'''<br\",\n",
       " 'لارستان',\n",
       " 'خلا',\n",
       " 'الانبار',\n",
       " 'حقلانیه',\n",
       " 'کربلاء',\n",
       " '|کربلا',\n",
       " \"کربلا'''\",\n",
       " 'الاخیضر',\n",
       " 'الاقیصر',\n",
       " 'الامیر',\n",
       " 'کربلا-',\n",
       " 'وادی\\u200cالسلا',\n",
       " 'بنه\\u200cسلاوه',\n",
       " 'شقلاوه|شقلاوه',\n",
       " 'شقلاوه',\n",
       " '<small>بعلاوه',\n",
       " '=اعلا',\n",
       " 'باتلاق',\n",
       " 'بلاروس|بلاروسی',\n",
       " 'لاکهار',\n",
       " 'اسلاوی\\u200cزب',\n",
       " 'یاروسلاو',\n",
       " 'یاروسلاول',\n",
       " 'لاستیک',\n",
       " 'ابتلا',\n",
       " 'لایلا',\n",
       " 'ولادیمیر-سوزدال',\n",
       " 'والاگرایی|سوپره\\u200cماتیس',\n",
       " 'اختلاف\\u200cنظر',\n",
       " 'عقلان',\n",
       " 'بلام',\n",
       " 'اخلاق-علم-و-ایدئولوژی-از-منظر-مارکس',\n",
       " 'کلاسیک|مارکس',\n",
       " \"'''مردم\\u200cسالار\",\n",
       " 'غیرانقلاب',\n",
       " 'مردم\\u200cسالار',\n",
       " 'ختلان',\n",
       " 'ختلانى',\n",
       " \"ختلانی'''\",\n",
       " 'فعالان',\n",
       " 'سالانه=',\n",
       " 'زلال',\n",
       " 'قوبلا',\n",
       " 'الاغ',\n",
       " 'مارگولاشویل',\n",
       " 'گرجستان|لار',\n",
       " 'زاکاتالا',\n",
       " 'تحت\\u200cالارض',\n",
       " 'لوگلا',\n",
       " 'آلا|قزل',\n",
       " 'آلاس',\n",
       " 'لاینفک',\n",
       " 'آلازان',\n",
       " 'فعالانه',\n",
       " 'لاریس',\n",
       " '|نام\\u200cرسمی=جلال\\u200cآباد',\n",
       " '|ولایت=ننگرهار',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    return bi_index[bigram]\n",
    "\n",
    "get_words_with_bigram('لا')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Rz7JnPLJneY"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای اضافه کردن یک سند به نمایه‌ها است.\n",
    "تابع\n",
    "add_document_to_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه،\n",
    "در صورت نبود آن سند در نمایه‌ها، آن را به نمایه‌ها اضافه می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3V8mUFHJneZ"
   },
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    global index, N, bi_index\n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.parse(docs_path)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        title = child.find('{http://www.mediawiki.org/xml/export-0.10/}title').text\n",
    "        doc_id = child.find('{http://www.mediawiki.org/xml/export-0.10/}id').text\n",
    "        text = child.find('{http://www.mediawiki.org/xml/export-0.10/}revision').find('{http://www.mediawiki.org/xml/export-0.10/}text').text\n",
    "        if doc_id != str(doc_num):\n",
    "            continue\n",
    "        title_words = prepare_text(title)\n",
    "        text_words = prepare_text(text)\n",
    "        N += 1\n",
    "        make_dict(index, bi_index, title_words, doc_id, 'title')\n",
    "        make_dict(index, bi_index, text_words, doc_id, 'text')\n",
    "\n",
    "add_document_to_indexes('data/Persian.xml', 3854)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI62QI7FJnec"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای حذف کردن یک سند از نمایه است.\n",
    "تابع\n",
    "delete_document_from_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه سند، آن سند را از نمایه‌ها حذف می‌کند.\n",
    "در صورتی که پس از حذف یک سند، \n",
    "یک کلمه دیگر در بین محتوای سند‌ها وجود نداشته‌باشد، آن کلمه باید از دیکشنری نمایه‌ی اصلی \n",
    "به طور کامل حذف شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ_8rPAxJneg"
   },
   "outputs": [],
   "source": [
    "def delete_bigrams(words):\n",
    "    for word in words:\n",
    "        if word in index.keys():\n",
    "            continue\n",
    "        word_bi = '$'+word+'$'\n",
    "        for i in range(len(word_bi)-1):\n",
    "            bi = word_bi[i] + word_bi[i+1]\n",
    "            if bi in bi_index.keys():\n",
    "                list_of_words = bi_index[bi]\n",
    "                if word in list_of_words:\n",
    "                    list_of_words.remove(word)\n",
    "\n",
    "def delete_from_dict(index, words, doc_id, part):\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in index.keys():\n",
    "            pass\n",
    "        else:\n",
    "            word_dict = index[word]\n",
    "            if doc_id not in word_dict.keys():\n",
    "                pass\n",
    "            else:\n",
    "                del word_dict[doc_id]\n",
    "                if not word_dict:\n",
    "                    del index[word]\n",
    "                    delete_bigrams(word)\n",
    "                else:\n",
    "                    index[word] = word_dict\n",
    "                    \n",
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    global index, N, bi_index\n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.parse(docs_path)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        title = child.find('{http://www.mediawiki.org/xml/export-0.10/}title').text\n",
    "        doc_id = child.find('{http://www.mediawiki.org/xml/export-0.10/}id').text\n",
    "        text = child.find('{http://www.mediawiki.org/xml/export-0.10/}revision').find('{http://www.mediawiki.org/xml/export-0.10/}text').text\n",
    "        if doc_id != str(doc_num):\n",
    "            continue\n",
    "        title_words = prepare_text(title)\n",
    "        text_words = prepare_text(text)\n",
    "        N -= 1\n",
    "        delete_from_dict(index, title_words, doc_id, 'title')\n",
    "        delete_from_dict(index, text_words, doc_id, 'text')\n",
    "        delete_bigrams(title_words)\n",
    "        delete_bigrams(text_words)\n",
    "\n",
    "# delete_document_from_indexes('data/Persian.xml', 3854)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL-19qvrJnek"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای ذخیره‌سازی نمایه‌ی اول است\n",
    "و نیازی به ذخیره‌سازی نمایه \n",
    "Bigram نیست \n",
    ".\n",
    "تابع \n",
    "save_index\n",
    "گرفتن مسیر فایل ذخیره کردن نمایه با نام \n",
    "destination\n",
    "نمایه ساخته‌شده را در این مسیر ذخیره می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UestOXpOJnel"
   },
   "outputs": [],
   "source": [
    "def save_index(destination):\n",
    "    import pickle\n",
    "    with open(destination + 'index.pkl', 'wb') as f:\n",
    "        pickle.dump(index, f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(destination + 'bi_index.pkl', 'wb') as f:\n",
    "        pickle.dump(bi_index, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_index('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPL51GrmJnep"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای بارگذاری نمایه از یک فایل است. تابع \n",
    "load_index\n",
    "با گرفتن مسیر فایل ذخیره شده نمایه با نام \n",
    "source\n",
    "نمایه را از این فایل بارگذاری می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahonOMFMJneq"
   },
   "outputs": [],
   "source": [
    "def load_index(source):\n",
    "    import pickle\n",
    "    with open(source + 'index.pkl', 'rb') as f:\n",
    "        main_index = pickle.load(f)\n",
    "    with open(source + 'bi_index.pkl', 'rb') as f:\n",
    "        bigram_index = pickle.load(f)\n",
    "    return main_index, bigram_index\n",
    "\n",
    "index, bi_index = load_index('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5JeSxd2Jnet"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (40 نمره) بخش سوم: جستجو وبازیابی اسناد</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این قسمت لازم است تا پرسمانی که از کاربر گرفته می‌شود در مجموعه اسناد نمایه شده، جستجو شود. جستجو به دو صورت بازیابی ترتیب دار در فضای برداری و بازیابی دقیق عبارت است. جستجو باید هم در عنوان سند صورت بگیرد هم در متن آن و در نهایت ترتیب اسناد بازگردانده شده بر اساس امتیازی‌ است که از جمع وزن‌دار امتیاز جست‌وجو در عنوان و جست‌وجو در متن به دست آمده‌است.\n",
    "(*امتیازی*)\n",
    "همچنین گاهی ممکن است پرسمان ارائه شده حاوی غلط املایی باشد، در این صورت لازم است تا ابتدا پرسمان را اصلاح کنید و سپس جستجو انجام شود. \n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "(*امتیازی*)\n",
    "اصلاح پرسمان\n",
    "</font>\n",
    "<br>\n",
    "اصلاح پرسمان ورودی: ممکن است پرسمان ورودی\n",
    "کاربر غلط املایی داشته باشد؛ در چنین مواردی برای هر لغت از پرسمان ورودی که در نمایه موجود  نیست ابتدا نزدیکترین لغات موجود در نمایه \n",
    "bigram\n",
    "(با استفاده از معیار فاصله جاکارد) \n",
    "انتخاب شده و سپس\n",
    "بهترین آنها با معیار \n",
    "edit distance\n",
    "نسبت به کلمه اصلی، جایگزین می‌شود. در صورتی که چند لغت فاصله برابری از لغت مورد نظر داشته باشند، می‌توانید یکی از آنها را به دلخواه انتخاب کنید.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "بازیابی ترتیب دار در فضای برداری tf-idf به روشهای ltn-lnn و ltc-lnc\n",
    "</font>\n",
    "<br>\n",
    " در این بخش پرسمان به صورت یک پرسمان کلی مطرح می‌شود و جست‌وجوی یک پرسمان بر روی هر دو بخش عنوان و متن صورت می‌گیرد و سپس نتیجه بر اساس امتیاز وزن‌دار جست و جو بر روی این دو بخش به ترتیب برگردانده می‌شود. وزن امتیاز جست‌وجو در عنوان نسبت به وزن امتیاز جست‌وجو در متن باید به عنوان پارامتر ورودی قابل تنظیم باشد اما در حالت پیش‌فرض آن را ۲ در نظر می‌گیریم. \n",
    "<br>\n",
    "برای هر پرسمان، پس از مشخص شدن روش امتیازدهی به عنوان ورودی\n",
    "(ltn-lnn\n",
    "و\n",
    "ltc-lnc)\n",
    "شما باید لیستی مرتب از شناسه اسناد بر اساس امتیاز کسب شده برگردانید که امتیازات بر اساس توضیحات بالا باید محاسبه شوند.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "جستجوی دقیق \n",
    "(phrasal search)\n",
    "</font>\n",
    "<br>\n",
    "این نوع جست‌وجو در قالب جست‌وجو‌های ترتیب‌دار قسمت قبل استفاده می‌شود. به این ترتیب که \n",
    "پرسمان ورودی ممکن است شامل تعدادی لغت و عبارات داخل گیومه باشد. اسناد بازیابی شده می‌بایست شامل عبارات داخل گیومه دقیقا به همان ترتیب و شکل آمده داخل گیومه باشند. \n",
    "<br>\n",
    "در صورت وجود چند عبارت داخل گیومه در پرسمان، ترتیب عبارات آمده داخل چند گیومه نسبت به هم لزومی ندارد حفظ شود. به \n",
    "عنوان نمونه برای پرسمان\n",
    "<br>\n",
    "\"q5 q4\" q3 \"q2 q1\"\n",
    "<br>\n",
    "سند\n",
    "<br>\n",
    "q3 q2 q1 q5 q4\n",
    "<br>\n",
    "مرتبط محسوب می‌شود. \n",
    "<br>\n",
    "جست‌وجو باید به این صورت باشد که ابتدا مجموعەی تمامی اسناد دارای عبارت‌های داخل گیومه پیدا می‌شود و سپس با استفاده از تمام لغات داخل پرسمان (شامل لغات داخل گیومه) بازیابی ترتیب دار با توضیحات آمده در قسمت قبل انجام شود.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UnpNX56Jneu"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای اصلاح پرسمان‌های ورودی است. تابع \n",
    "correct_query\n",
    "پرسمان کاربر  \n",
    "(query)\n",
    "را به عنوان ورودی می‌گیرد و در صورتی که کلماتی در پرسمان داخل واژه‌نامه‌ی نمایه وجود نداشته باشد آن کلمات را به شکل توضیح داده‌شده در بخش اصلاح پرسمان، با کلمات نزدیک به آن در واژه‌نامه جایگزین می‌کند و پرسمان اصلاح‌شده را برمی‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5kybrGDJnev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query Words After Preprocess by HAZM: ['شلا', 'حالا', 'برسه', 'درسک', 'شد']\n",
      "Corrected Query Words: ['شلا', 'حالا', 'بره', 'درس', 'شد']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['شلا', 'حالا', 'بره', 'درس', 'شد']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correct_query(query):\n",
    "    query_words = prepare_text(query)\n",
    "    correct_words = []\n",
    "    for j, word in enumerate(query_words):\n",
    "        correct_words.append(query_words[j])\n",
    "        if word not in index.keys():\n",
    "            union = {}\n",
    "            bi_only_in_wrong_word = 0\n",
    "            intersection = {}\n",
    "            score = {}\n",
    "            bi_word = '$'+word+'$'\n",
    "            for i in range(len(bi_word)-1):\n",
    "                bi = bi_word[i] + bi_word[i+1]\n",
    "                if bi in bi_index.keys():\n",
    "                    for sim_word in bi_index[bi]:\n",
    "                        if sim_word in union.keys():\n",
    "                            union[sim_word] += 1\n",
    "                            intersection[sim_word] += 1\n",
    "                        else:\n",
    "                            union[sim_word] = 1\n",
    "                            intersection[sim_word] = 1\n",
    "                else:\n",
    "                    bi_only_in_wrong_word += 1\n",
    "            for w in intersection.keys():\n",
    "                score[w] = intersection[w]/(union[w]+bi_only_in_wrong_word+len(w)+1-intersection[w])\n",
    "                \n",
    "            sorted_candidates = {k: v for k, v in sorted(score.items(), key=lambda item: item[1], reverse=True)}\n",
    "            correct_words[j] = next(iter(sorted_candidates))\n",
    "    print(\"Original Query Words After Preprocess by HAZM:\", query_words)\n",
    "    print(\"Corrected Query Words:\", correct_words)\n",
    "    return correct_words\n",
    "\n",
    "correct_query(\"شلام حالا برسهان درسک شد\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9HMqDMZJney"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان کلی اختصاص دارد. تابع \n",
    "search\n",
    "به عنوان اولین پارامتر پرسمان \n",
    "(query)\n",
    "را گرفته و جست و جو را روی آن انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد.\n",
    "پارامتر سوم \n",
    "(weight)\n",
    "که یک عدد اعشاری است نسبت وزن امتیاز جست‌وجو در عنوان به امتیاز جست‌وجو در متن را مشخص می‌کند. که به طور پیش‌فرض این مقدار برابر ۲ است. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsgz32PDJnez",
    "outputId": "b3b6ed43-794b-42e1-bc03-ee3e0134318b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query Words After Preprocess by HAZM: ['باشگاه', 'فوتبال', 'اروپا']\n",
      "Corrected Query Words: ['باشگاه', 'فوتبال', 'اروپا']\n",
      "Original Query Words After Preprocess by HAZM: ['اروپا']\n",
      "Corrected Query Words: ['اروپا']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6753,\n",
       " 6417,\n",
       " 6978,\n",
       " 5236,\n",
       " 5509,\n",
       " 6798,\n",
       " 3666,\n",
       " 6870,\n",
       " 4094,\n",
       " 6900,\n",
       " 7134,\n",
       " 4530,\n",
       " 6694,\n",
       " 6885,\n",
       " 4537]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2):\n",
    "    global maximum_number_of_results\n",
    "    import re\n",
    "    import numpy as np\n",
    "    match = ''\n",
    "    exact_query_words = []\n",
    "    query_words = correct_query(query)\n",
    "    \n",
    "    # finding docs having exact words -for words between \"\".\n",
    "    while True:\n",
    "        match = re.search('\"(.*?)\"', query)\n",
    "        if match is None:\n",
    "            break\n",
    "        exact_query = match.group(1)\n",
    "        query = query.replace('\"'+exact_query+'\"', '')\n",
    "        exact_query_words += correct_query(exact_query)\n",
    "    exact_words_docs_list = []\n",
    "    for word in exact_query_words:\n",
    "        word_list = get_posting_list(word)\n",
    "        exact_words_docs_list.append(list(word_list.keys()))\n",
    "    list_of_docs = []\n",
    "    if len(exact_words_docs_list) > 0:\n",
    "        list1 = exact_words_docs_list[0]\n",
    "        for list2 in exact_words_docs_list[1:]:\n",
    "            list1 = intersection(list1, list2)\n",
    "        list_of_docs = list1\n",
    "    \n",
    "    # finding number of each word's repeat in query -needed for building query's vector\n",
    "    import math\n",
    "    query_words_no_repeat = set()\n",
    "    n_repeat_query = {}\n",
    "    for word in query_words:\n",
    "        if word in n_repeat_query.keys():\n",
    "            n_repeat_query[word] += 1\n",
    "        else:\n",
    "            n_repeat_query[word] = 1\n",
    "        query_words_no_repeat.add(word)\n",
    "    query_words = list(query_words_no_repeat)\n",
    "    \n",
    "    # building query vector\n",
    "    query_vector = [0. for word in query_words]\n",
    "    for i, word in enumerate(query_words):\n",
    "        list1 = get_posting_list(word)\n",
    "        tf = n_repeat_query[word]\n",
    "        idf = len(list(list1.keys()))\n",
    "        tf = math.log10(1+tf)\n",
    "        idf = math.log10(N/idf)\n",
    "        score = tf * idf\n",
    "        query_vector[i] = score\n",
    "        \n",
    "    query_vector = np.asarray(query_vector)\n",
    "    if method == \"ltc-lnc\":\n",
    "        query_vector /= np.sqrt(np.sum(np.asarray(query_vector+1e-10)**2))\n",
    "    \n",
    "    # if there was no exact word request -no word between \"\"- all docs having at least one of query words are selected\n",
    "    set_of_docs = set()\n",
    "    if len(list_of_docs) <= 0:\n",
    "        for word in query_words:\n",
    "            list1 = get_posting_list(word)\n",
    "            set_of_docs.update(list(list1.keys()))\n",
    "        list_of_docs = list(set_of_docs)\n",
    "    \n",
    "    # building docs vectors\n",
    "    docs_vectors = {}\n",
    "    if len(list_of_docs) > 0:\n",
    "        for doc in list_of_docs:\n",
    "            docs_vectors[doc] = [0. for word in query_words]\n",
    "            for i, word in enumerate(query_words):\n",
    "                list1 = get_posting_list(word)\n",
    "                if doc in list1.keys():\n",
    "                    if 'title' in list1[doc].keys():\n",
    "                        tf_title = len(list1[doc]['title'])\n",
    "                    else:\n",
    "                        tf_title = 0\n",
    "                    if 'text' in list1[doc].keys():\n",
    "                        tf_text = len(list1[doc]['text'])\n",
    "                    else:\n",
    "                        tf_text = 0\n",
    "                    tf_title = math.log10(1+tf_title)\n",
    "                    tf_text = math.log10(1+tf_text)\n",
    "                    score_title = tf_title\n",
    "                    score_text = tf_text\n",
    "                    score = score_text + weight * score_title\n",
    "                    docs_vectors[doc][i] = score            \n",
    "            docs_vectors[doc] = np.asarray(docs_vectors[doc])\n",
    "            if method == \"ltc-lnc\":\n",
    "                docs_vectors[doc] /= np.sqrt(np.sum(np.asarray(docs_vectors[doc]+1e-10)**2))\n",
    "    \n",
    "    # using heap for safe results\n",
    "    heap = []\n",
    "    score_of_docs = {}\n",
    "    for doc in list_of_docs:\n",
    "        score_of_docs[doc] = sum(docs_vectors[doc]*query_vector)\n",
    "        heapq.heappush(heap, (score_of_docs[doc], doc))\n",
    "    relevant_docs = []\n",
    "    relevant_docs = heapq.nlargest(min(len(heap), maximum_number_of_results), heap)\n",
    "    for i, doc in enumerate(relevant_docs):\n",
    "        relevant_docs[i] = int(doc[1])\n",
    "    return relevant_docs\n",
    "\n",
    "#     sorted_docs = sorted(score_of_docs, key=score_of_docs.get, reverse=True)\n",
    "#     for doc in sorted_docs:\n",
    "#         relevant_docs.append(int(doc))\n",
    "#     return relevant_docs[:min(len(relevant_docs), maximum_number_of_results)]\n",
    "\n",
    "maximum_number_of_results = 15\n",
    "search('باشگاه فوتبال \"اروپایی\"', \"ltn-lnn\", 3)\n",
    "# search('سواحل دریای سرخ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTXhfuP0Jne5"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان بر اساس بخش اختصاص دارد. تابع \n",
    "detailed_search\n",
    "به عنوان دو پارامتر اول پرسمان بر روی عنوان \n",
    "(title_query)\n",
    "و پرسمان بر روی متن\n",
    "(text_query)\n",
    "را گرفته و جست و جو را روی آن‌ها انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmOjg1gYJne6",
    "outputId": "88f5ed0f-3ac6-4bba-c148-8e7aa4de09c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query Words After Preprocess by HAZM: ['عجایب', 'هفت\\u200cگانه']\n",
      "Corrected Query Words: ['عجایب', 'هفت\\u200cگانه']\n",
      "Original Query Words After Preprocess by HAZM: ['چشمگیر', 'بنا', 'تاریخ', 'جه']\n",
      "Corrected Query Words: ['چشمگیر', 'بنا', 'تاریخ', 'جه']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3854]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    global maximum_number_of_results\n",
    "    import re\n",
    "    import numpy as np\n",
    "    \n",
    "    # finding docs with exact words of title query in their title\n",
    "    match = ''\n",
    "    exact_title_query_words = []\n",
    "    title_query_words = correct_query(title_query)\n",
    "    while True:\n",
    "        match = re.search('\"(.*?)\"', title_query)\n",
    "        if match is None:\n",
    "            break\n",
    "        exact_query = match.group(1)\n",
    "        title_query = title_query.replace('\"'+exact_query+'\"', '')\n",
    "        exact_title_query_words += correct_query(exact_query)\n",
    "    exact_title_words_docs_list = []\n",
    "    for word in exact_title_query_words:\n",
    "        word_list = get_posting_list(word)\n",
    "        for doc in word_list.keys():\n",
    "            if 'title' in word_list[doc].keys():\n",
    "                exact_title_words_docs_list.append(doc)\n",
    "    list_of_title_docs = []\n",
    "    if len(exact_title_words_docs_list) > 0:\n",
    "        list1 = exact_title_words_docs_list[0]\n",
    "        for list2 in exact_title_words_docs_list[1:]:\n",
    "            list1 = intersection(list1, list2)\n",
    "        list_of_title_docs = list1\n",
    "        \n",
    "    # finding docs with exact words of text query in their text\n",
    "    match = ''\n",
    "    exact_text_query_words = []\n",
    "    text_query_words = correct_query(text_query)\n",
    "    while True:\n",
    "        match = re.search('\"(.*?)\"', text_query)\n",
    "        if match is None:\n",
    "            break\n",
    "        exact_query = match.group(1)\n",
    "        text_query = text_query.replace('\"'+exact_query+'\"', '')\n",
    "        exact_text_query_words += correct_query(exact_query)\n",
    "    exact_text_words_docs_list = []\n",
    "    for word in exact_text_query_words:\n",
    "        word_list = get_posting_list(word)\n",
    "        for doc in word_list.keys():\n",
    "            if 'text' in word_list[doc].keys():\n",
    "                exact_text_words_docs_list.append(doc)    \n",
    "    list_of_text_docs = []\n",
    "    if len(exact_text_words_docs_list) > 0:\n",
    "        list1 = exact_text_words_docs_list[0]\n",
    "        for list2 in exact_text_words_docs_list[1:]:\n",
    "            list1 = intersection(list1, list2)\n",
    "        list_of_text_docs = list1\n",
    "    \n",
    "    # all docs having both exact words of title and text\n",
    "    list_of_docs = intersection(list_of_text_docs, list_of_title_docs)\n",
    "    \n",
    "    import math\n",
    "    # finding number of repeat of title query words\n",
    "    title_query_words_no_repeat = set()\n",
    "    n_repeat_title_query = {}\n",
    "    for word in title_query_words:\n",
    "        if word in n_repeat_title_query.keys():\n",
    "            n_repeat_title_query[word] += 1\n",
    "        else:\n",
    "            n_repeat_title_query[word] = 1\n",
    "        title_query_words_no_repeat.add(word)\n",
    "    title_query_words = list(title_query_words_no_repeat)\n",
    "    \n",
    "    # finding number of repeat of text query words\n",
    "    text_query_words_no_repeat = set()\n",
    "    n_repeat_text_query = {}\n",
    "    for word in text_query_words:\n",
    "        if word in n_repeat_text_query.keys():\n",
    "            n_repeat_text_query[word] += 1\n",
    "        else:\n",
    "            n_repeat_text_query[word] = 1\n",
    "        text_query_words_no_repeat.add(word)\n",
    "    text_query_words = list(text_query_words_no_repeat)\n",
    "    \n",
    "    # title query vector\n",
    "    query_words = title_query_words\n",
    "    query_vector = [0. for word in query_words]\n",
    "    for i, word in enumerate(query_words):\n",
    "        list1 = get_posting_list(word)\n",
    "        tf = n_repeat_title_query[word]\n",
    "        idf = len(list(list1.keys()))\n",
    "        tf = math.log10(1+tf)\n",
    "        idf = math.log10(N/idf)\n",
    "        score = tf * idf\n",
    "        query_vector[i] = score\n",
    "    query_vector = np.asarray(query_vector)\n",
    "    if method == \"ltc-lnc\":\n",
    "        query_vector /= np.sqrt(np.sum(np.asarray(query_vector+1e-10)**2))\n",
    "    title_query_vector = query_vector\n",
    "    \n",
    "    # text query vector\n",
    "    query_words = text_query_words\n",
    "    query_vector = [0. for word in query_words]\n",
    "    for i, word in enumerate(query_words):\n",
    "        list1 = get_posting_list(word)\n",
    "        tf = n_repeat_text_query[word]\n",
    "        idf = len(list(list1.keys()))\n",
    "        tf = math.log10(1+tf)\n",
    "        idf = math.log10(N/idf)\n",
    "        score = tf * idf\n",
    "        query_vector[i] = score\n",
    "    query_vector = np.asarray(query_vector)\n",
    "    if method == \"ltc-lnc\":\n",
    "        query_vector /= np.sqrt(np.sum(np.asarray(query_vector+1e-10)**2))\n",
    "    text_query_vector = query_vector\n",
    "    \n",
    "    # if no docs having exact words of title and text we search through all docs having at least one word of queries\n",
    "    set_of_docs = set()\n",
    "    if len(list_of_docs) <= 0:\n",
    "        for word in title_query_words:\n",
    "            list1 = get_posting_list(word)\n",
    "            set_of_docs.update(list(list1.keys()))\n",
    "        for word in text_query_words:\n",
    "            list1 = get_posting_list(word)\n",
    "            set_of_docs.update(list(list1.keys()))\n",
    "        list_of_docs = list(set_of_docs)\n",
    "    \n",
    "    # docs title vectors\n",
    "    docs_vectors = {}\n",
    "    query_words = title_query_words\n",
    "    if len(list_of_docs) > 0:\n",
    "        for doc in list_of_docs:\n",
    "            docs_vectors[doc] = [0. for word in query_words]\n",
    "            for i, word in enumerate(query_words):\n",
    "                list1 = get_posting_list(word)\n",
    "                if doc in list1.keys():\n",
    "                    if 'title' in list1[doc].keys():\n",
    "                        tf_title = len(list1[doc]['title'])\n",
    "                    else:\n",
    "                        tf_title = 0\n",
    "                    tf_title = math.log10(1+tf_title)\n",
    "                    score_title = tf_title\n",
    "                    score = score_title\n",
    "                    docs_vectors[doc][i] = score            \n",
    "            docs_vectors[doc] = np.asarray(docs_vectors[doc])\n",
    "            if method == \"ltc-lnc\":\n",
    "                docs_vectors[doc] /= np.sqrt(np.sum(np.asarray(docs_vectors[doc]+1e-10)**2))\n",
    "    score_of_docs = {}\n",
    "    for doc in list_of_docs:\n",
    "        score_of_docs[doc] = sum(docs_vectors[doc]*title_query_vector)\n",
    "    \n",
    "    # docs text vectors\n",
    "    docs_vectors = {}\n",
    "    query_words = text_query_words\n",
    "    if len(list_of_docs) > 0:\n",
    "        for doc in list_of_docs:\n",
    "            docs_vectors[doc] = [0. for word in query_words]\n",
    "            for i, word in enumerate(query_words):\n",
    "                list1 = get_posting_list(word)\n",
    "                if doc in list1.keys():\n",
    "                    if 'text' in list1[doc].keys():\n",
    "                        tf_text = len(list1[doc]['text'])\n",
    "                    else:\n",
    "                        tf_text = 0\n",
    "                    tf_text = math.log10(1+tf_text)\n",
    "                    score_text = tf_text\n",
    "                    score = score_text\n",
    "                    docs_vectors[doc][i] = score            \n",
    "            docs_vectors[doc] = np.asarray(docs_vectors[doc])\n",
    "            if method == \"ltc-lnc\":\n",
    "                docs_vectors[doc] /= np.sqrt(np.sum(np.asarray(docs_vectors[doc]+1e-10)**2))\n",
    "    heap = []\n",
    "    # we add the score of text to the score of title\n",
    "    # we use heap for safe search\n",
    "    for doc in list_of_docs:\n",
    "        score_of_docs[doc] += sum(docs_vectors[doc]*text_query_vector)\n",
    "        heapq.heappush(heap, (score_of_docs[doc], doc))\n",
    "    relevant_docs = heapq.nlargest(min(len(heap), maximum_number_of_results), heap)\n",
    "    for i, doc in enumerate(relevant_docs):\n",
    "        relevant_docs[i] = int(doc[1])\n",
    "    return relevant_docs\n",
    "        \n",
    "#     sorted_docs = sorted(score_of_docs, key=score_of_docs.get, reverse=True)\n",
    "#     relevant_docs = []\n",
    "#     for doc in sorted_docs:\n",
    "#         relevant_docs.append(int(doc))\n",
    "#     return relevant_docs[:min(len(relevant_docs), maximum_number_of_results)]\n",
    "\n",
    "maximum_number_of_results = 1\n",
    "detailed_search('عجایب هفت‌گانه', 'چشمگیرترین بناهای تاریخی جهان', \"ltc-lnc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpAO-xvEJne-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (20 نمره) بخش چهارم: ارزیابی سیستم</div>\n",
    "</font>\n",
    "<hr>\n",
    "سیستم شما باید قادر باشد با استفاده از معیارهای\n",
    "<ol>\n",
    "<li>\n",
    "MAP\n",
    "</li>\n",
    "<li>\n",
    "F-Measure\n",
    "</li>\n",
    "<li>\n",
    "R-Precision\n",
    "</li>\n",
    "<li>\n",
    "NDCG\n",
    "</li>\n",
    "</ol>\n",
    "نتایج را ارزیابی کند. برای ارزیابی تعدادی پرسمان و نتایج آنها در اختیار شما قرار گرفته است که باید پاسخ سیستم‌تان به پرسمان ها را با نتایج متناظر هر پرسمان ارزیابی و مقایسه کنید. در صورتی که کل پرسمان در یک خط آمده بود به این معنی است که پرسمان کلی است و تابع\n",
    "search \n",
    "باید برای آن فراخوانی شود و در صورتی که پرسمان در  دو خط آمده بود، خط اول پرسمان عنوان و خط دوم پرسمان متن خواهد بود و باید تابع\n",
    "detailed_search\n",
    "را برای آن فراخوانی کنید و نتیجه را ارزیابی کنید.\n",
    "<br>\n",
    "توجه کنید که این چهار معیار را جداگانه و مستقل از بقیه نیز بتوانید حساب کنید. حداکثر سند بازیابی شده را ۱۵ قرار دهید.    \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l52cf_R-Jne_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در قطعه کد بخش زیر به ازای هر معیار یک تابع آمده است که به عنوان ورودی شماره پرسمان را می‌گیرد و با خواندن پرسمان و لیست مرتب سند‌های مرتبط با آن از فایل‌های مربوطه، جستجوی پرسمان را با توجه به نوع پرسمان انجام می‌دهد، نتیجه را ارزیابی کرده و مقدار محاسبه شده معیار را بر می‌گرداند.\n",
    "در صورتی که در ورودی به جای شماره پرسمان رشته‌ی\n",
    "all\n",
    "آمده بود ارزیابی باید بر روی تمامی اسناد صورت گیرد و میانگین مقادیر معیار ازیابی برای همه پرسمان‌ها به عنوان خروجی برگردانده شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqaOk4ESJnfA"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "\n",
    "maximum_number_of_results = 15\n",
    "def R_Precision(query_id='all'):\n",
    "    \n",
    "    # sample code for read query! You can change anyway\n",
    "    if query_id == 'all':\n",
    "        results = []\n",
    "        for file in glob.glob('./data/queries/*.txt'):\n",
    "            relevant = 0\n",
    "            total = 0\n",
    "            with open(file, 'r', encoding='utf-8-sig') as query_file:\n",
    "                import re\n",
    "                name = query_file.name.replace('\\\\', '/')\n",
    "                match = re.search('queries/(.*?).txt', name)\n",
    "                query_id = match.group(1)\n",
    "                query = query_file.readline()\n",
    "                try:\n",
    "                    query2 = query_file.readline()\n",
    "                    if query2 is \"\":\n",
    "                        raise IOError\n",
    "                    result = detailed_search(query, query2)\n",
    "                except:\n",
    "                    result = search(query)\n",
    "                with open('./data/relevance/%s.txt'%(query_id,)) as baseline_file:\n",
    "                    baseline = list(map(int, baseline_file.read().replace('[', '').replace(']', '').replace(' ', '').split(',')))\n",
    "                for res in result:\n",
    "                    if res in baseline:\n",
    "                        relevant += 1\n",
    "                    total += 1\n",
    "                results.append(relevant/len(baseline))\n",
    "        result = sum(results)/len(results)\n",
    "    else:\n",
    "        with open('./data/queries/%s.txt'%(query_id,), encoding='utf-8-sig') as query_file:\n",
    "            query = query_file.readline()\n",
    "            total = 0\n",
    "            relevant = 0\n",
    "            try:\n",
    "                query2 = query_file.readline()\n",
    "                if query2 is \"\":\n",
    "                    raise IOError\n",
    "                result = detailed_search(query, query2)\n",
    "            except:\n",
    "                result = search(query)\n",
    "            with open('./data/relevance/%s.txt'%(query_id,)) as baseline_file:\n",
    "                baseline = list(map(int, baseline_file.read().replace('[', '').replace(']', '').replace(' ', '').split(',')))\n",
    "            for res in result:\n",
    "                if res in baseline:\n",
    "                    relevant += 1\n",
    "                total += 1\n",
    "            result = relevant / len(baseline)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def F_measure(query_id='all'):\n",
    "    \n",
    "    # sample code for read query! You can change anyway\n",
    "    if query_id == 'all':\n",
    "        results = []\n",
    "        for file in glob.glob('./data/queries/*.txt'):\n",
    "            relevant = 0\n",
    "            total = 0\n",
    "            with open(file, 'r', encoding='utf-8-sig') as query_file:\n",
    "                import re\n",
    "                name = query_file.name.replace('\\\\', '/')\n",
    "                match = re.search('queries/(.*?).txt', name)\n",
    "                query_id = match.group(1)\n",
    "                query = query_file.readline()\n",
    "                try:\n",
    "                    query2 = query_file.readline()\n",
    "                    if query2 is \"\":\n",
    "                        raise IOError\n",
    "                    result = detailed_search(query, query2)\n",
    "                except:\n",
    "                    result = search(query)\n",
    "                with open('./data/relevance/%s.txt'%(query_id,), encoding='utf-8-sig') as baseline_file:\n",
    "                    baseline = list(map(int, baseline_file.read().replace('[', '').replace(']', '').replace(' ', '').split(',')))\n",
    "                for res in result:\n",
    "                    if res in baseline:\n",
    "                        relevant += 1\n",
    "                    total += 1\n",
    "                precision = relevant/total\n",
    "                relevant = 0\n",
    "                total = 0\n",
    "                for res in baseline:\n",
    "                    if res in result:\n",
    "                        relevant += 1\n",
    "                    total += 1\n",
    "                recall = relevant/total\n",
    "                results.append(2*(precision*recall)/(precision+recall+1e-15))\n",
    "        result = sum(results)/len(results)\n",
    "    else:\n",
    "        with open('./data/queries/%s.txt'%(query_id,), encoding='utf-8-sig') as query_file:\n",
    "            query = query_file.readline()\n",
    "            total = 0\n",
    "            relevant = 0\n",
    "            try:\n",
    "                query2 = query_file.readline()\n",
    "                if query2 is \"\":\n",
    "                    raise IOError\n",
    "                result = detailed_search(query, query2)\n",
    "            except:\n",
    "                result = search(query)\n",
    "            with open('./data/relevance/%s.txt'%(query_id,), encoding='utf-8-sig') as baseline_file:\n",
    "                baseline = list(map(int, baseline_file.read().replace('[', '').replace(']', '').replace(' ', '').split(',')))\n",
    "            \n",
    "            for res in result:\n",
    "                if res in baseline:\n",
    "                    relevant += 1\n",
    "                total += 1\n",
    "            precision = relevant/total\n",
    "            relevant = 0\n",
    "            total = 0\n",
    "            for res in baseline:\n",
    "                if res in result:\n",
    "                    relevant += 1\n",
    "                total += 1\n",
    "            recall = relevant/total\n",
    "            result = 2*(precision*recall)/(precision+recall+1e-15)\n",
    "    return result\n",
    "\n",
    "def MAP(query_id='all'):\n",
    "    \n",
    "    # sample code for read query! You can change anyway\n",
    "    if query_id == 'all':\n",
    "        results = []\n",
    "        for file in glob.glob('./data/queries/*.txt'):\n",
    "            relevant = 0\n",
    "            total = 0\n",
    "            ps = []\n",
    "            with open(file, 'r', encoding='utf-8-sig') as query_file:\n",
    "                import re\n",
    "                name = query_file.name.replace('\\\\', '/')\n",
    "                match = re.search('queries/(.*?).txt', name)\n",
    "                query_id = match.group(1)\n",
    "                query = query_file.readline()\n",
    "                try:\n",
    "                    query2 = query_file.readline()\n",
    "                    if query2 is \"\":\n",
    "                        raise IOError\n",
    "                    result = detailed_search(query, query2)\n",
    "                except:\n",
    "                    result = search(query)\n",
    "                with open('./data/relevance/%s.txt'%(query_id,), encoding='utf-8-sig') as baseline_file:\n",
    "                    baseline = list(map(int, baseline_file.read().replace('[', '').replace(']', '').replace(' ', '').split(',')))\n",
    "                for res in result:\n",
    "                    total += 1\n",
    "                    if res in baseline:\n",
    "                        relevant += 1\n",
    "                        ps.append(relevant/total)\n",
    "                results.append(sum(ps)/(len(ps)+1e-15))\n",
    "        result = sum(results)/len(results)\n",
    "    else:\n",
    "        with open('./data/queries/%s.txt'%(query_id,), encoding='utf-8-sig') as query_file:\n",
    "            query = query_file.readline()\n",
    "            total = 0\n",
    "            relevant = 0\n",
    "            ps = []\n",
    "            try:\n",
    "                query2 = query_file.readline()\n",
    "                if query2 is \"\":\n",
    "                    raise IOError\n",
    "                result = detailed_search(query, query2)\n",
    "            except:\n",
    "                result = search(query)\n",
    "            with open('./data/relevance/%s.txt'%(query_id,), encoding='utf-8-sig') as baseline_file:\n",
    "                baseline = list(map(int, baseline_file.read().replace('[', '').replace(']', '').replace(' ', '').split(',')))\n",
    "            for res in result:\n",
    "                total += 1\n",
    "                if res in baseline:\n",
    "                    relevant += 1\n",
    "                    ps.append(relevant/total)\n",
    "            result = sum(ps)/(len(ps)+1e-15)\n",
    "    return result\n",
    "\n",
    "def NDCG(relevance_of_ground_truth_docs, query_id='all'):\n",
    "    \n",
    "    # sample code for read query! You can change anyway\n",
    "    if query_id == 'all':\n",
    "        results = []\n",
    "        for file in glob.glob('./data/queries/*.txt'):\n",
    "            relevance = 0\n",
    "            ideal_relevance = 0\n",
    "            ps = []\n",
    "            with open(file, 'r', encoding='utf-8-sig') as query_file:\n",
    "                import re\n",
    "                name = query_file.name.replace('\\\\', '/')\n",
    "                match = re.search('queries/(.*?).txt', name)\n",
    "                query_id = match.group(1)\n",
    "                query = query_file.readline()\n",
    "                try:\n",
    "                    query2 = query_file.readline()\n",
    "                    if query2 is \"\":\n",
    "                        raise IOError\n",
    "                    result = detailed_search(query, query2)\n",
    "                except:\n",
    "                    result = search(query)\n",
    "                with open('./data/relevance/%s.txt'%(query_id,), encoding='utf-8-sig') as baseline_file:\n",
    "                    baseline = list(map(int, baseline_file.read().replace('[', '').replace(']', '').replace(' ', '').split(',')))\n",
    "                print(query)\n",
    "                print(result)\n",
    "                print('Enter the relevance of these docs:')\n",
    "                relevance_of_retrieved_docs = list(map(float, input().split()))\n",
    "                for i, res in enumerate(result):\n",
    "                    rel = relevance_of_retrieved_docs[i]\n",
    "                    relevance += rel/((math.log2(i+1)) if i > 0 else 1)\n",
    "                for i, res in enumerate(baseline):\n",
    "                    rel = relevance_of_ground_truth_docs[i]\n",
    "                    ideal_relevance += rel/((math.log2(i+1)) if i > 0 else 1)\n",
    "                    \n",
    "                results.append(relevance/ideal_relevance)\n",
    "        result = sum(results)/len(results)\n",
    "    else:\n",
    "        with open('./data/queries/%s.txt'%(query_id,), encoding='utf-8-sig') as query_file:\n",
    "            query = query_file.readline()\n",
    "            relevance = 0\n",
    "            ideal_relevance = 0\n",
    "            ps = []\n",
    "            try:\n",
    "                query2 = query_file.readline()\n",
    "                if query2 is \"\":\n",
    "                    raise IOError\n",
    "                result = detailed_search(query, query2)\n",
    "            except:\n",
    "                result = search(query)\n",
    "            with open('./data/relevance/%s.txt'%(query_id,), encoding='utf-8-sig') as baseline_file:\n",
    "                baseline = list(map(int, baseline_file.read().replace('[', '').replace(']', '').replace(' ', '').split(',')))\n",
    "            print(query)\n",
    "            print(result)\n",
    "            print('Enter the relevance of these docs:')\n",
    "            relevance_of_retrieved_docs = list(map(float, input().split()))\n",
    "            for i, res in enumerate(result):\n",
    "                rel = relevance_of_retrieved_docs[i]\n",
    "                relevance += rel/((math.log2(i+1)) if i > 0 else 1)\n",
    "            for i, res in enumerate(baseline):\n",
    "                rel = relevance_of_ground_truth_docs[i]\n",
    "                ideal_relevance += rel/((math.log2(i+1)) if i > 0 else 1)\n",
    "            result = relevance/ideal_relevance\n",
    "                    \n",
    "    return result\n",
    "\n",
    "# print(R_Precision('all'))\n",
    "# print(F_measure('all'))\n",
    "# print(MAP('all'))\n",
    "\n",
    "# we should input the relevance of searched docs + the relevance of docs acclaimed ideal\n",
    "# print(NDCG([1 for i in range(50)], 'all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUs450l5JnfD"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>نکات پایانی</div>\n",
    "</font>\n",
    "<hr>\n",
    "۱- سیستم را به صورت بهینه پیاده سازی کنید تا در زمان کمتری بارگذاری و نمایه سازی و … را انجام دهد.\n",
    "<br>\n",
    "۲- فایل‌های \n",
    "ipynb\n",
    "و پایتون \n",
    "پاسخ تمرین را (بدون داده‌ها) به صورت فایل فشرده در کوئرا بارگذاری کنید.\n",
    "<br>\n",
    "۳- اشکالات خود از فاز اول پروژه را در زیر پست مربوط به این تمرین بپرسید.\n",
    "<br>\n",
    "۴- نام فایل ارسالی به صورت Project1-StudentNumber باشد.\n",
    "<br>\n",
    "۵- موعد تحویل تمرین تا ساعت ۲۳:۵۹ پانزدهم فروردین می‌باشد و جریمەی تأخیر مطابق با قوانینی که در سایت درس قرار داده شدەاست ، خواهد بود.\n",
    "<br>\n",
    "۶- در صورت مشاهده تقلب، طبق قوانین دانشکده با شما برخورد خواهد شد.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ testing 'prepare_text' =============================================\n",
      "prepared text is : ['کتاب', 'مناسب', 'نوشته_شوند', 'در', 'راستا', 'ارتقا', 'سطح', 'آموز', 'کشور', 'تلاش', 'زیاد', 'صور', 'می\\u200cگیرد'] with length: 13\n",
      "\n",
      "============ testing 'get_posting_list' =========================================\n",
      "number of ocurrences of the word فکری  in documents =  160\n",
      "docs with the word: ['3014', '3099', '3103', '3119', '3197', '3217', '3229', '3359', '3373', '3404', '3429', '3654', '3699', '3776', '3777', '3798', '3826', '3879', '3897', '3938', '4002', '4062', '4248', '4275', '4321', '4335', '4382', '4388', '4391', '4398', '4400', '4460', '4636', '4650', '4671', '4718', '4743', '4805', '4843', '4864', '5192', '5308', '5381', '5486', '5554', '5571', '5707', '5720', '5820', '5967', '6014', '6052', '6088', '6229', '6294', '6417', '6418', '6475', '6522', '6568', '6572', '6609', '6629', '6634', '6710', '6735', '6749', '6752', '6753', '6791', '6847', '6848', '6907', '6931', '6944', '6959', '6973', '7133']\n",
      "\n",
      "============ testing 'get_words_with_bigram' ====================================\n",
      "returned list length: 3127\n",
      "checking word هیلاندراس : True\n",
      "\n",
      "============ testing 'doc_remove' ================================================\n",
      "length of posting list for word فکری before removing doc 3014 : 78\n",
      "length of posting list for word فکری after removing doc 3014 : 77\n",
      "length of posting list for word هیلاندراس before removing doc 6752 : 1\n",
      "length of posting list for word هیلاندراس after removing doc 6752 : 0\n",
      "\n",
      "============ testing correct bigram removal ========================================\n",
      "returned list length: 3099\n",
      "checking word هیلاندراس : False\n",
      "\n",
      "============ testing 'doc_add' ================================================\n",
      "length of posting list for word فکری before adding doc 3014 : 76\n",
      "number of ocurrences for  فکری : 154\n",
      "length of posting list for word فکری after adding doc 3014 : 77\n",
      "number of ocurrences for  فکری : 159\n",
      "length of posting list for word هیلاندراس before adding doc 6752 : 0\n",
      "length of posting list for word هیلاندراس after adding doc 6752 : 1\n",
      "returned list length: 3127\n",
      "checking word هیلاندراس : True\n",
      "\n",
      "============ testing save and load methods ========================================\n",
      "length of posting list for word فکری before saving: 78\n",
      "number of ocurrences for  فکری : 160\n",
      "length of posting list for word فکری after loading: 78\n",
      "number of ocurrences for  فکری : 160\n"
     ]
    }
   ],
   "source": [
    "word1 = 'فکری'\n",
    "doc_id = 3014\n",
    "\n",
    "\n",
    "word2 = 'هیلاندراس'\n",
    "doc_id2 = 6752\n",
    "bigram = 'لا'\n",
    "\n",
    "def get_count (l):\n",
    "    i = [1 for _,t in l.items() for q in t['text']]\n",
    "    j = [1 for _,t in l.items() if 'title' in t.keys() for q in t['title']]\n",
    "    return len (i) + len(j)\n",
    "\n",
    "\n",
    "def test_prepare_text():\n",
    "    print (\"\\n============ testing 'prepare_text' =============================================\")\n",
    "    raw_text = \"کتابهای مناسبی نوشته شوند ! در راستای ارتقای . سطح آموزش کشور ؟ تلاش‌های زیادی صورت می‌گیرد\"\n",
    "    prepared_text = prepare_text(raw_text)\n",
    "    \n",
    "    print(\"prepared text is :\", prepared_text , \"with length:\" , len (prepared_text))\n",
    "    \n",
    "test_prepare_text()\n",
    "\n",
    "def test_get_posting_list():\n",
    "    \n",
    "    print (\"\\n============ testing 'get_posting_list' =========================================\")\n",
    "    \n",
    "    prepared_text = prepare_text(word1)[0]\n",
    "    posting_list = get_posting_list(prepared_text)\n",
    "    # posting_list = {3014:{'title':[...] , 'text':[...]}}\n",
    "    \n",
    "    \n",
    "#     print (\"posting list for input\" , prepared_text, \"is :\", posting_list , \"with length:\" , len (posting_list))\n",
    "\n",
    "    ###########################################CHANGED#####################################################\n",
    "    try:\n",
    "        print (\"number of ocurrences of the word\", word1 , \" in documents = \", get_count (posting_list))\n",
    "        print ('docs with the word:' , sorted (list (posting_list.keys())))\n",
    "    except KeyError:\n",
    "        print (\"number of ocurrences of the word\", word1 , \" in documents = \", 0)\n",
    "        print ('docs with the word:' , [])\n",
    "    ########################################END OF CHANGE###################################################    \n",
    "        \n",
    "test_get_posting_list()\n",
    "\n",
    "\n",
    "def test_bigram():\n",
    "    print (\"\\n============ testing 'get_words_with_bigram' ====================================\")\n",
    "    \n",
    "    words_with_bigram = get_words_with_bigram(bigram)\n",
    "    print (\"returned list length:\" , len (words_with_bigram))\n",
    "\n",
    "    print (\"checking word\" , word2 , \":\", word2 in words_with_bigram)\n",
    "    \n",
    "\n",
    "check_bigram = True\n",
    "if check_bigram:\n",
    "    test_bigram()\n",
    "\n",
    "def test_doc_remove():\n",
    "    \n",
    "    print (\"\\n============ testing 'doc_remove' ================================================\")\n",
    "    \n",
    "    ############################################CHANGED############################################################\n",
    "    try:\n",
    "        posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "        print (\"length of posting list for word\" , word1 , \"before removing doc\" , doc_id, \":\" , len (posting_list))\n",
    "    except KeyError:\n",
    "        print (\"length of posting list for word\" , word1 , \"before removing doc\" , doc_id, \":\" , 0)\n",
    "        \n",
    "    delete_document_from_indexes('data/Persian.xml', doc_id)\n",
    "    \n",
    "    try:\n",
    "        posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "        print (\"length of posting list for word\" , word1 , \"after removing doc\" , doc_id, \":\" , len (posting_list))\n",
    "    except:\n",
    "        print (\"length of posting list for word\" , word1 , \"after removing doc\" , doc_id, \":\" , 0)\n",
    "\n",
    "    try:\n",
    "        posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "        print (\"length of posting list for word\" , word2 , \"before removing doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    except KeyError:\n",
    "        print (\"length of posting list for word\" , word2 , \"before removing doc\" , doc_id2, \":\" , 0)\n",
    "    \n",
    "    delete_document_from_indexes('data/Persian.xml', doc_id2)\n",
    "    \n",
    "    try:\n",
    "        posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "        print (\"length of posting list for word\" , word2 , \"after removing doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    except KeyError:\n",
    "        print (\"length of posting list for word\" , word2 , \"after removing doc\" , doc_id2, \":\" , 0)\n",
    "    ############################################END OF CHANGE########################################################\n",
    "    \n",
    "test_doc_remove()\n",
    "\n",
    "def test_doc_remove_bigram():\n",
    "    print (\"\\n============ testing correct bigram removal ========================================\")\n",
    "    words_with_bigram = get_words_with_bigram(bigram)\n",
    "    print (\"returned list length:\" , len (words_with_bigram))\n",
    "    print (\"checking word\" , word2 , \":\", word2 in words_with_bigram)\n",
    "    \n",
    "\n",
    "if check_bigram:\n",
    "    test_doc_remove_bigram()\n",
    "\n",
    "def test_doc_add():\n",
    "    print (\"\\n============ testing 'doc_add' ================================================\")\n",
    "    \n",
    "    ###########################################CHANGED#####################################################\n",
    "    try:\n",
    "        posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "        print (\"length of posting list for word\" , word1 , \"before adding doc\" , doc_id, \":\" , len (posting_list))\n",
    "        print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    except KeyError:\n",
    "        print (\"length of posting list for word\" , word1 , \"before adding doc\" , doc_id, \":\" , 0)\n",
    "        print (\"number of ocurrences for \", word1, \":\", 0)\n",
    "    \n",
    "    add_document_to_indexes('data/Persian.xml', doc_id)\n",
    "    \n",
    "    try:\n",
    "        posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "        print (\"length of posting list for word\" , word1 , \"after adding doc\" , doc_id, \":\" , len (posting_list))\n",
    "        print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    except KeyError:\n",
    "        print (\"length of posting list for word\" , word1 , \"after adding doc\" , doc_id, \":\" , 0)\n",
    "        print (\"number of ocurrences for \", word1, \":\", 0)\n",
    "    \n",
    "    try:\n",
    "        posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "        print (\"length of posting list for word\" , word2 , \"before adding doc\" , doc_id2, \":\" , len (posting_list))\n",
    "               \n",
    "    except KeyError:\n",
    "        print (\"length of posting list for word\" , word2 , \"before adding doc\" , doc_id2, \":\" , 0)\n",
    "    \n",
    "    add_document_to_indexes('data/Persian.xml', doc_id2)\n",
    "    \n",
    "    try:\n",
    "        posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "        print (\"length of posting list for word\" , word2 , \"after adding doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    except KeyError:\n",
    "        print (\"length of posting list for word\" , word2 , \"after adding doc\" , doc_id2, \":\" , 0)\n",
    "    \n",
    "    ########################################END OF CHANGE####################################################           \n",
    "    \n",
    "test_doc_add()\n",
    "\n",
    "\n",
    "def test_doc_add_bigram():\n",
    "    \n",
    "    words_with_bigram = get_words_with_bigram(bigram)\n",
    "    print (\"returned list length:\" , len (words_with_bigram))\n",
    "    print (\"checking word\" , word2 , \":\", word2 in words_with_bigram)\n",
    "\n",
    "if check_bigram:\n",
    "    test_doc_add_bigram()\n",
    "    \n",
    "def test_save_and_load ():\n",
    "    print (\"\\n============ testing save and load methods ========================================\")\n",
    "    \n",
    "    ##################################CHANGED#################################\n",
    "    destination = \"./\"\n",
    "    ################################END OF CHANGE#############################\n",
    "\n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"before saving:\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "    save_index(destination)\n",
    "    ##################################CHANGED#################################\n",
    "    index, bi_index = load_index(destination)\n",
    "    ##################################CHANGED#################################\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"after loading:\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "test_save_and_load ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query Words After Preprocess by HAZM: ['ابذار', 'های', 'فظا', 'و', 'پیشرفته', 'ناصا']\n",
      "Corrected Query Words: ['ار', 'های', 'فا', 'و', 'پیشرفته', 'نا']\n",
      "['ار', 'های', 'فا', 'و', 'پیشرفته', 'نا']\n",
      "Original Query Words After Preprocess by HAZM: ['سیاره', 'های', 'بزرگ', 'منظومه', 'شمس']\n",
      "Corrected Query Words: ['سیاره', 'های', 'بزرگ', 'منظومه', 'شمس']\n",
      "Original Query Words After Preprocess by HAZM: ['منظومه', 'شمس']\n",
      "Corrected Query Words: ['منظومه', 'شمس']\n",
      "[4255, 5724, 3903, 3416, 4627, 3414, 4659, 6629, 3411, 6266, 3415, 4376, 3359, 4259, 3373]\n",
      "Original Query Words After Preprocess by HAZM: ['فهرس', 'شهر', 'ایر']\n",
      "Corrected Query Words: ['فهرس', 'شهر', 'ایر']\n",
      "Original Query Words After Preprocess by HAZM: ['اس', 'گیل', 'شهرس', 'لنگرود']\n",
      "Corrected Query Words: ['اس', 'گیل', 'شهرس', 'لنگرود']\n",
      "[3760, 5236, 5264, 3874, 5381, 4275, 4094, 6394, 4339, 6159, 4907, 6694, 4896, 6369, 4074]\n"
     ]
    }
   ],
   "source": [
    "def test_correct_query():\n",
    "\n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    query = 'ابذار های فظایی و پیشرفته ناصا'\n",
    "    ##################################\n",
    "    \n",
    "    result = correct_query(query)\n",
    "    print(result)\n",
    "\n",
    "test_correct_query()\n",
    "\n",
    "def test_search():\n",
    "\n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    query = 'سیاره های بزرگ \"منظومه شمسی\"'\n",
    "    method = \"ltc-lnc\"\n",
    "    ##################################\n",
    "\n",
    "    relevant_docs = search(query, method)\n",
    "    print(relevant_docs)\n",
    "\n",
    "test_search()\n",
    "\n",
    "def test_detailed_search():\n",
    "    \n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    title_query = 'فهرست شهرهای ایران'\n",
    "    text_query = 'استان گیلان شهرستان لنگرود'\n",
    "    ##################################\n",
    "\n",
    "    relevant_docs = detailed_search(title_query, text_query)\n",
    "    print(relevant_docs)\n",
    "\n",
    "test_detailed_search()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "doc:\tall\n",
      "Original Query Words After Preprocess by HAZM: ['باشگاه', 'فوتبال', 'اروپا']\n",
      "Corrected Query Words: ['باشگاه', 'فوتبال', 'اروپا']\n",
      "Original Query Words After Preprocess by HAZM: ['باشگاه', 'فوتبال']\n",
      "Corrected Query Words: ['باشگاه', 'فوتبال']\n",
      "Original Query Words After Preprocess by HAZM: ['انتخاب', 'نمایندگ', 'مجلس', 'ایالت', 'در', 'آمریکا']\n",
      "Corrected Query Words: ['انتخاب', 'نمایندگ', 'مجلس', 'ایالت', 'در', 'آمریکا']\n",
      "Original Query Words After Preprocess by HAZM: ['سیاره', 'های', 'بزرگ', 'منظومه', 'شمس']\n",
      "Corrected Query Words: ['سیاره', 'های', 'بزرگ', 'منظومه', 'شمس']\n",
      "Original Query Words After Preprocess by HAZM: ['منظومه', 'شمس']\n",
      "Corrected Query Words: ['منظومه', 'شمس']\n",
      "Original Query Words After Preprocess by HAZM: ['جاذبه', 'گردشگر', 'در', 'اس', 'کردس']\n",
      "Corrected Query Words: ['جاذبه', 'گردشگر', 'در', 'اس', 'کردس']\n",
      "Original Query Words After Preprocess by HAZM: ['هیتلر', 'در', 'جنگ', 'جهان', 'اول']\n",
      "Corrected Query Words: ['هیتلر', 'در', 'جنگ', 'جهان', 'اول']\n",
      "Original Query Words After Preprocess by HAZM: ['برنده', 'جایزه', 'به', 'فیل', 'در', 'جشنواره']\n",
      "Corrected Query Words: ['برنده', 'جایزه', 'به', 'فیل', 'در', 'جشنواره']\n",
      "Original Query Words After Preprocess by HAZM: ['تاریخچه', 'هنر', 'نمایش', 'در', 'ایر']\n",
      "Corrected Query Words: ['تاریخچه', 'هنر', 'نمایش', 'در', 'ایر']\n",
      "Original Query Words After Preprocess by HAZM: ['در', 'بیمار', 'افسردگ']\n",
      "Corrected Query Words: ['در', 'بیمار', 'افسردگ']\n",
      "Original Query Words After Preprocess by HAZM: ['ابزار', 'های', 'فضا', 'و', 'پیشرفته', 'ناسا']\n",
      "Corrected Query Words: ['ابزار', 'های', 'فضا', 'و', 'پیشرفته', 'ناسا']\n",
      "Original Query Words After Preprocess by HAZM: ['کشور', 'دارا', 'نف', 'در', 'خاورمینا']\n",
      "Corrected Query Words: ['کشور', 'دارا', 'نف', 'در', 'خا']\n",
      "Original Query Words After Preprocess by HAZM: ['پایتخ', 'کشور', 'حوزه', 'خلیج', 'فارس']\n",
      "Corrected Query Words: ['پایتخ', 'کشور', 'حوزه', 'خلیج', 'فارس']\n",
      "Original Query Words After Preprocess by HAZM: ['خلیج', 'فارس']\n",
      "Corrected Query Words: ['خلیج', 'فارس']\n",
      "Original Query Words After Preprocess by HAZM: ['مسابق', 'فوتبال', 'المپیک']\n",
      "Corrected Query Words: ['مسابق', 'فوتبال', 'المپیک']\n",
      "Original Query Words After Preprocess by HAZM: ['کتاب', 'های', 'برگزیده', 'کودک', 'و', 'نوجو']\n",
      "Corrected Query Words: ['کتاب', 'های', 'برگزیده', 'کودک', 'و', 'نوجو']\n",
      "Original Query Words After Preprocess by HAZM: ['طبیع', 'دامنه', 'کوه', 'ایران']\n",
      "Corrected Query Words: ['طبیع', 'دامنه', 'کوه', 'ایران']\n",
      "Original Query Words After Preprocess by HAZM: ['کشور', 'عضو', 'اتحادیه', 'آفریقا']\n",
      "Corrected Query Words: ['کشور', 'عضو', 'اتحادیه', 'آفریقا']\n",
      "Original Query Words After Preprocess by HAZM: ['سواحل', 'دریا', 'سرخ']\n",
      "Corrected Query Words: ['سواحل', 'دریا', 'سرخ']\n",
      "Original Query Words After Preprocess by HAZM: ['مطالعه', 'علو', 'اجتماع', 'در', 'دانشگاه']\n",
      "Corrected Query Words: ['مطالعه', 'علو', 'اجتماع', 'در', 'دانشگاه']\n",
      "Original Query Words After Preprocess by HAZM: ['علو', 'اجتماع']\n",
      "Corrected Query Words: ['علو', 'اجتماع']\n",
      "Original Query Words After Preprocess by HAZM: ['تاریخ', 'علو', 'اجتماع', 'در', 'اروپا']\n",
      "Corrected Query Words: ['تاریخ', 'علو', 'اجتماع', 'در', 'اروپا']\n",
      "Original Query Words After Preprocess by HAZM: ['زندگ', 'حیو', 'وحش']\n",
      "Corrected Query Words: ['زندگ', 'حیو', 'وحش']\n",
      "Original Query Words After Preprocess by HAZM: ['جنگل', 'های', 'بلوط', 'ایر']\n",
      "Corrected Query Words: ['جنگل', 'های', 'بلوط', 'ایر']\n",
      "R_Precision:\t0.51\n",
      "Original Query Words After Preprocess by HAZM: ['باشگاه', 'فوتبال', 'اروپا']\n",
      "Corrected Query Words: ['باشگاه', 'فوتبال', 'اروپا']\n",
      "Original Query Words After Preprocess by HAZM: ['باشگاه', 'فوتبال']\n",
      "Corrected Query Words: ['باشگاه', 'فوتبال']\n",
      "Original Query Words After Preprocess by HAZM: ['انتخاب', 'نمایندگ', 'مجلس', 'ایالت', 'در', 'آمریکا']\n",
      "Corrected Query Words: ['انتخاب', 'نمایندگ', 'مجلس', 'ایالت', 'در', 'آمریکا']\n",
      "Original Query Words After Preprocess by HAZM: ['سیاره', 'های', 'بزرگ', 'منظومه', 'شمس']\n",
      "Corrected Query Words: ['سیاره', 'های', 'بزرگ', 'منظومه', 'شمس']\n",
      "Original Query Words After Preprocess by HAZM: ['منظومه', 'شمس']\n",
      "Corrected Query Words: ['منظومه', 'شمس']\n",
      "Original Query Words After Preprocess by HAZM: ['جاذبه', 'گردشگر', 'در', 'اس', 'کردس']\n",
      "Corrected Query Words: ['جاذبه', 'گردشگر', 'در', 'اس', 'کردس']\n",
      "Original Query Words After Preprocess by HAZM: ['هیتلر', 'در', 'جنگ', 'جهان', 'اول']\n",
      "Corrected Query Words: ['هیتلر', 'در', 'جنگ', 'جهان', 'اول']\n",
      "Original Query Words After Preprocess by HAZM: ['برنده', 'جایزه', 'به', 'فیل', 'در', 'جشنواره']\n",
      "Corrected Query Words: ['برنده', 'جایزه', 'به', 'فیل', 'در', 'جشنواره']\n",
      "Original Query Words After Preprocess by HAZM: ['تاریخچه', 'هنر', 'نمایش', 'در', 'ایر']\n",
      "Corrected Query Words: ['تاریخچه', 'هنر', 'نمایش', 'در', 'ایر']\n",
      "Original Query Words After Preprocess by HAZM: ['در', 'بیمار', 'افسردگ']\n",
      "Corrected Query Words: ['در', 'بیمار', 'افسردگ']\n",
      "Original Query Words After Preprocess by HAZM: ['ابزار', 'های', 'فضا', 'و', 'پیشرفته', 'ناسا']\n",
      "Corrected Query Words: ['ابزار', 'های', 'فضا', 'و', 'پیشرفته', 'ناسا']\n",
      "Original Query Words After Preprocess by HAZM: ['کشور', 'دارا', 'نف', 'در', 'خاورمینا']\n",
      "Corrected Query Words: ['کشور', 'دارا', 'نف', 'در', 'خا']\n",
      "Original Query Words After Preprocess by HAZM: ['پایتخ', 'کشور', 'حوزه', 'خلیج', 'فارس']\n",
      "Corrected Query Words: ['پایتخ', 'کشور', 'حوزه', 'خلیج', 'فارس']\n",
      "Original Query Words After Preprocess by HAZM: ['خلیج', 'فارس']\n",
      "Corrected Query Words: ['خلیج', 'فارس']\n",
      "Original Query Words After Preprocess by HAZM: ['مسابق', 'فوتبال', 'المپیک']\n",
      "Corrected Query Words: ['مسابق', 'فوتبال', 'المپیک']\n",
      "Original Query Words After Preprocess by HAZM: ['کتاب', 'های', 'برگزیده', 'کودک', 'و', 'نوجو']\n",
      "Corrected Query Words: ['کتاب', 'های', 'برگزیده', 'کودک', 'و', 'نوجو']\n",
      "Original Query Words After Preprocess by HAZM: ['طبیع', 'دامنه', 'کوه', 'ایران']\n",
      "Corrected Query Words: ['طبیع', 'دامنه', 'کوه', 'ایران']\n",
      "Original Query Words After Preprocess by HAZM: ['کشور', 'عضو', 'اتحادیه', 'آفریقا']\n",
      "Corrected Query Words: ['کشور', 'عضو', 'اتحادیه', 'آفریقا']\n",
      "Original Query Words After Preprocess by HAZM: ['سواحل', 'دریا', 'سرخ']\n",
      "Corrected Query Words: ['سواحل', 'دریا', 'سرخ']\n",
      "Original Query Words After Preprocess by HAZM: ['مطالعه', 'علو', 'اجتماع', 'در', 'دانشگاه']\n",
      "Corrected Query Words: ['مطالعه', 'علو', 'اجتماع', 'در', 'دانشگاه']\n",
      "Original Query Words After Preprocess by HAZM: ['علو', 'اجتماع']\n",
      "Corrected Query Words: ['علو', 'اجتماع']\n",
      "Original Query Words After Preprocess by HAZM: ['تاریخ', 'علو', 'اجتماع', 'در', 'اروپا']\n",
      "Corrected Query Words: ['تاریخ', 'علو', 'اجتماع', 'در', 'اروپا']\n",
      "Original Query Words After Preprocess by HAZM: ['زندگ', 'حیو', 'وحش']\n",
      "Corrected Query Words: ['زندگ', 'حیو', 'وحش']\n",
      "Original Query Words After Preprocess by HAZM: ['جنگل', 'های', 'بلوط', 'ایر']\n",
      "Corrected Query Words: ['جنگل', 'های', 'بلوط', 'ایر']\n",
      "F_measure  :\t0.52\n",
      "Original Query Words After Preprocess by HAZM: ['باشگاه', 'فوتبال', 'اروپا']\n",
      "Corrected Query Words: ['باشگاه', 'فوتبال', 'اروپا']\n",
      "Original Query Words After Preprocess by HAZM: ['باشگاه', 'فوتبال']\n",
      "Corrected Query Words: ['باشگاه', 'فوتبال']\n",
      "Original Query Words After Preprocess by HAZM: ['انتخاب', 'نمایندگ', 'مجلس', 'ایالت', 'در', 'آمریکا']\n",
      "Corrected Query Words: ['انتخاب', 'نمایندگ', 'مجلس', 'ایالت', 'در', 'آمریکا']\n",
      "Original Query Words After Preprocess by HAZM: ['سیاره', 'های', 'بزرگ', 'منظومه', 'شمس']\n",
      "Corrected Query Words: ['سیاره', 'های', 'بزرگ', 'منظومه', 'شمس']\n",
      "Original Query Words After Preprocess by HAZM: ['منظومه', 'شمس']\n",
      "Corrected Query Words: ['منظومه', 'شمس']\n",
      "Original Query Words After Preprocess by HAZM: ['جاذبه', 'گردشگر', 'در', 'اس', 'کردس']\n",
      "Corrected Query Words: ['جاذبه', 'گردشگر', 'در', 'اس', 'کردس']\n",
      "Original Query Words After Preprocess by HAZM: ['هیتلر', 'در', 'جنگ', 'جهان', 'اول']\n",
      "Corrected Query Words: ['هیتلر', 'در', 'جنگ', 'جهان', 'اول']\n",
      "Original Query Words After Preprocess by HAZM: ['برنده', 'جایزه', 'به', 'فیل', 'در', 'جشنواره']\n",
      "Corrected Query Words: ['برنده', 'جایزه', 'به', 'فیل', 'در', 'جشنواره']\n",
      "Original Query Words After Preprocess by HAZM: ['تاریخچه', 'هنر', 'نمایش', 'در', 'ایر']\n",
      "Corrected Query Words: ['تاریخچه', 'هنر', 'نمایش', 'در', 'ایر']\n",
      "Original Query Words After Preprocess by HAZM: ['در', 'بیمار', 'افسردگ']\n",
      "Corrected Query Words: ['در', 'بیمار', 'افسردگ']\n",
      "Original Query Words After Preprocess by HAZM: ['ابزار', 'های', 'فضا', 'و', 'پیشرفته', 'ناسا']\n",
      "Corrected Query Words: ['ابزار', 'های', 'فضا', 'و', 'پیشرفته', 'ناسا']\n",
      "Original Query Words After Preprocess by HAZM: ['کشور', 'دارا', 'نف', 'در', 'خاورمینا']\n",
      "Corrected Query Words: ['کشور', 'دارا', 'نف', 'در', 'خا']\n",
      "Original Query Words After Preprocess by HAZM: ['پایتخ', 'کشور', 'حوزه', 'خلیج', 'فارس']\n",
      "Corrected Query Words: ['پایتخ', 'کشور', 'حوزه', 'خلیج', 'فارس']\n",
      "Original Query Words After Preprocess by HAZM: ['خلیج', 'فارس']\n",
      "Corrected Query Words: ['خلیج', 'فارس']\n",
      "Original Query Words After Preprocess by HAZM: ['مسابق', 'فوتبال', 'المپیک']\n",
      "Corrected Query Words: ['مسابق', 'فوتبال', 'المپیک']\n",
      "Original Query Words After Preprocess by HAZM: ['کتاب', 'های', 'برگزیده', 'کودک', 'و', 'نوجو']\n",
      "Corrected Query Words: ['کتاب', 'های', 'برگزیده', 'کودک', 'و', 'نوجو']\n",
      "Original Query Words After Preprocess by HAZM: ['طبیع', 'دامنه', 'کوه', 'ایران']\n",
      "Corrected Query Words: ['طبیع', 'دامنه', 'کوه', 'ایران']\n",
      "Original Query Words After Preprocess by HAZM: ['کشور', 'عضو', 'اتحادیه', 'آفریقا']\n",
      "Corrected Query Words: ['کشور', 'عضو', 'اتحادیه', 'آفریقا']\n",
      "Original Query Words After Preprocess by HAZM: ['سواحل', 'دریا', 'سرخ']\n",
      "Corrected Query Words: ['سواحل', 'دریا', 'سرخ']\n",
      "Original Query Words After Preprocess by HAZM: ['مطالعه', 'علو', 'اجتماع', 'در', 'دانشگاه']\n",
      "Corrected Query Words: ['مطالعه', 'علو', 'اجتماع', 'در', 'دانشگاه']\n",
      "Original Query Words After Preprocess by HAZM: ['علو', 'اجتماع']\n",
      "Corrected Query Words: ['علو', 'اجتماع']\n",
      "Original Query Words After Preprocess by HAZM: ['تاریخ', 'علو', 'اجتماع', 'در', 'اروپا']\n",
      "Corrected Query Words: ['تاریخ', 'علو', 'اجتماع', 'در', 'اروپا']\n",
      "Original Query Words After Preprocess by HAZM: ['زندگ', 'حیو', 'وحش']\n",
      "Corrected Query Words: ['زندگ', 'حیو', 'وحش']\n",
      "Original Query Words After Preprocess by HAZM: ['جنگل', 'های', 'بلوط', 'ایر']\n",
      "Corrected Query Words: ['جنگل', 'های', 'بلوط', 'ایر']\n",
      "MAP        :\t0.42\n",
      "Original Query Words After Preprocess by HAZM: ['باشگاه', 'فوتبال', 'اروپا']\n",
      "Corrected Query Words: ['باشگاه', 'فوتبال', 'اروپا']\n",
      "Original Query Words After Preprocess by HAZM: ['باشگاه', 'فوتبال']\n",
      "Corrected Query Words: ['باشگاه', 'فوتبال']\n",
      "\"باشگاه فوتبال\" اروپایی\n",
      "[6753, 6418, 6417, 6978, 5381, 5236, 5509, 6798, 7063, 3666, 6870, 4094, 6900, 7134, 4530]\n",
      "Enter the relevance of these docs:\n",
      "q\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'q'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-854c335a9e97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;31m###############################################END OF CHANGE############################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-9c78088a59b0>\u001b[0m in \u001b[0;36mNDCG\u001b[1;34m(relevance_of_ground_truth_docs, query_id)\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Enter the relevance of these docs:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                 \u001b[0mrelevance_of_retrieved_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                     \u001b[0mrel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelevance_of_retrieved_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'q'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    python_open\n",
    "    print(\"Already done!\")\n",
    "except NameError:\n",
    "    python_open = open\n",
    "    def open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None):\n",
    "        encoding=\"utf-8\"\n",
    "        return python_open(file, mode=mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=closefd, opener=opener)\n",
    "##################################\n",
    "## Do not change this part\n",
    "##################################\n",
    "test_docs = ['all', 1, 2, 3]\n",
    "functions = {'R_Precision':R_Precision, 'F_measure':F_measure, 'MAP':MAP, 'NDCG': NDCG}\n",
    "##################################\n",
    "\n",
    "for doc in test_docs:\n",
    "    print(\"{}\\ndoc:\\t{}\".format('-'*30, doc))\n",
    "    for f in functions.keys():\n",
    "        \n",
    "        ##################################################CHANGED##############################################\n",
    "        if f != 'NDCG':\n",
    "            out = functions[f](doc)\n",
    "        else:\n",
    "            out = functions[f]([1 for i in range(40)], doc)\n",
    "        ###############################################END OF CHANGE############################################\n",
    "        \n",
    "        print(\"{:11}:\\t{:.2f}\".format(f, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "R_Precision:\n",
      "1:\t0.94\tFalse\n",
      "2:\t0.44\tTrue\n",
      "3:\t0.00\tTrue\n",
      "------------------------------\n",
      "F_measure:\n",
      "1:\t0.97\tTrue\n",
      "2:\t0.62\tTrue\n",
      "3:\t0.00\tTrue\n",
      "------------------------------\n",
      "MAP:\n",
      "1:\t1.00\tFalse\n",
      "2:\t1.00\tFalse\n",
      "3:\t0.00\tTrue\n",
      "------------------------------\n",
      "NDCG:\n",
      "\"باشگاه فوتبال\" اروپایی\n",
      "[6753, 7134, 6978, 7136, 4530, 6798, 6885, 5381, 6900, 4537, 5509, 6794, 4094, 6417, 3666]\n",
      "Enter the relevance of these docs:\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "## Do not change this part\n",
    "##################################\n",
    "test_docs = [1, 2, 3]\n",
    "rels = [\n",
    "    [6753, 7134, 6978, 7136, 4530, 6798, 6885, 5381, 6900, 4537, 5509, 6794, 4094, 6417, 3666, 5967],\n",
    "    [6753, 5509, 4718, 6798, 6850, 6417, 6978, 6871],\n",
    "    list(range(20))\n",
    "]\n",
    "outputs = [{'R_Precision': 1.0, 'F_measure': 0.967741935483871, 'MAP': 0.9375, 'NDCG': 0.9635640110263509},\n",
    "           {'R_Precision': 0.4444444444444444, 'F_measure': 0.6153846153846153, 'MAP': 0.4444444444444444, 'NDCG': 0.6313802022799658},\n",
    "           {'R_Precision': 0.0, 'F_measure': 0.0, 'MAP': 0.0, 'NDCG': 0.0}]\n",
    "\n",
    "functions = {'R_Precision':R_Precision, 'F_measure':F_measure, 'MAP':MAP, 'NDCG': NDCG}\n",
    "##################################\n",
    "idx = 0 \n",
    "\n",
    "ds = detailed_search\n",
    "s = search\n",
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    return rels[idx][:15]\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2):\n",
    "    return rels[idx][:15]\n",
    "\n",
    "for f in functions.keys():\n",
    "    print(\"{}\\n{}:\".format('-'*30, f))\n",
    "    idx = 0\n",
    "    maximum_number_of_results = len(rels[idx])\n",
    "    for doc in test_docs:   \n",
    "        #################################################CHANGED##############################################\n",
    "        if f != \"NDCG\":\n",
    "            out = functions[f](doc)\n",
    "        else:\n",
    "            out = functions[f]([1 for i in range(50)], doc)\n",
    "        ###############################################END OF CHANGE##########################################\n",
    "        expected = outputs[idx][f]\n",
    "        print(\"{}:\\t{:.2f}\\t{}\".format(doc, out, abs(out-expected)<1e-3))\n",
    "        idx += 1\n",
    "\n",
    "detailed_search = ds\n",
    "search = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2AOY8hCJnfE"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B titr\" size=30>\n",
    "<p></p>\n",
    "<font color=#FF7500> \n",
    "موفق باشید\n",
    ":)\n",
    "<br>\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1Spring99_ژخحغ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
